{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Time Series Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from fastdtw import fastdtw\n",
    "from statistics import mean, pstdev\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pygal\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "from dtaidistance import dtw, dtw_visualisation as dtwvis\n",
    "from scipy import stats\n",
    "from dtaidistance import clustering\n",
    "from timeit import default_timer as timer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "working_directory = \"2GB-9N\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_subdirectories(directory=\"\"):\n",
    "    subdirectories = []\n",
    "    p = Path(f\"./../{working_directory}/{directory}\")\n",
    "    for item in p.glob('*/'):\n",
    "        if item.suffix not in (['.csv', '.zip']):\n",
    "            subdirectories.append(directory + \"/\" + item.name)\n",
    "    return subdirectories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "all_directories = get_subdirectories()\n",
    "nodes_directories = [x for x in all_directories if \"node\" in x]\n",
    "\n",
    "data_directories = []\n",
    "data_directories_groups = []\n",
    "for directory in nodes_directories:\n",
    "    cur_node_subdirectories = get_subdirectories(directory)\n",
    "    data_directories.append(cur_node_subdirectories)\n",
    "\n",
    "data_directories_groups = data_directories\n",
    "data_directories = [item for sublist in data_directories for item in sublist]\n",
    "\n",
    "function_names = data_directories_groups[1]\n",
    "function_names = list(map(lambda x: x[8:], data_directories_groups[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### All functions in `experiments_data`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- avgNetProfitGroupedBySoldDate\n",
      "- avgNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- avgNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- avgWholeSaleCostGroupedBySoldDate\n",
      "- countNetProfitGroupedBySoldDate\n",
      "- countNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- countNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- countWholeSaleCostGroupedBySoldDate\n",
      "- filterCatalogSalesWhereProfitNegative\n",
      "- filterCatalogSalesWhereProfitNegativeAndYearAfter2000\n",
      "- filterCatalogSalesWhereYearAfter2000\n",
      "- filterStoreSalesWhereProfitNegative\n",
      "- filterStoreSalesWhereProfitNegativeAndYearAfter2000\n",
      "- filterStoreSalesWhereYearAfter2000\n",
      "- maxNetProfitGroupedBySoldDate\n",
      "- maxNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- maxNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- maxWholeSaleCostGroupedBySoldDate\n",
      "- minNetProfitGroupedBySoldDate\n",
      "- minNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- minNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- minWholeSaleCostGroupedBySoldDate\n",
      "- summaryNetProfitGroupedBySoldDate\n",
      "- summaryWholeSaleCostGroupedBySoldDate\n",
      "- sumNetProfitGroupedBySoldDate\n",
      "- sumNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- sumNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- sumWholeSaleCostGroupedBySoldDate\n"
     ]
    }
   ],
   "source": [
    "for function in function_names:\n",
    "    print(f\"- {function[1:]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aggregating labels by functions names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        function_name  \\\n0                       avgNetProfitGroupedBySoldDate   \n1    avgNetProfitGroupedBySoldDateWhereProfitNegative   \n2     avgNetProfitGroupedBySoldDateWhereYearAfter2000   \n3                   avgWholeSaleCostGroupedBySoldDate   \n4                     countNetProfitGroupedBySoldDate   \n5   countNetProfitGroupedBySoldDateWhereProfitNega...   \n6   countNetProfitGroupedBySoldDateWhereYearAfter2000   \n7                 countWholeSaleCostGroupedBySoldDate   \n8               filterCatalogSalesWhereProfitNegative   \n9   filterCatalogSalesWhereProfitNegativeAndYearAf...   \n10               filterCatalogSalesWhereYearAfter2000   \n11                filterStoreSalesWhereProfitNegative   \n12  filterStoreSalesWhereProfitNegativeAndYearAfte...   \n13                 filterStoreSalesWhereYearAfter2000   \n14                      maxNetProfitGroupedBySoldDate   \n15   maxNetProfitGroupedBySoldDateWhereProfitNegative   \n16    maxNetProfitGroupedBySoldDateWhereYearAfter2000   \n17                  maxWholeSaleCostGroupedBySoldDate   \n18                      minNetProfitGroupedBySoldDate   \n19   minNetProfitGroupedBySoldDateWhereProfitNegative   \n20    minNetProfitGroupedBySoldDateWhereYearAfter2000   \n21                  minWholeSaleCostGroupedBySoldDate   \n22                      sumNetProfitGroupedBySoldDate   \n23   sumNetProfitGroupedBySoldDateWhereProfitNegative   \n24    sumNetProfitGroupedBySoldDateWhereYearAfter2000   \n25                  sumWholeSaleCostGroupedBySoldDate   \n26                  summaryNetProfitGroupedBySoldDate   \n27              summaryWholeSaleCostGroupedBySoldDate   \n\n                            label  \n0                     aggregation  \n1         aggregation, filtration  \n2   aggregation, filtration, join  \n3                     aggregation  \n4                     aggregation  \n5         aggregation, filtration  \n6   aggregation, filtration, join  \n7                     aggregation  \n8                      filtration  \n9                filtration, join  \n10               filtration, join  \n11                     filtration  \n12               filtration, join  \n13               filtration, join  \n14                    aggregation  \n15        aggregation, filtration  \n16  aggregation, filtration, join  \n17                    aggregation  \n18                    aggregation  \n19        aggregation, filtration  \n20  aggregation, filtration, join  \n21                    aggregation  \n22                    aggregation  \n23        aggregation, filtration  \n24  aggregation, filtration, join  \n25                    aggregation  \n26                    aggregation  \n27                    aggregation  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>function_name</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>avgNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>avgNetProfitGroupedBySoldDateWhereProfitNegative</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>avgNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>avgWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>countNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>countNetProfitGroupedBySoldDateWhereProfitNega...</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>countNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>countWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>filterCatalogSalesWhereProfitNegative</td>\n      <td>filtration</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>filterCatalogSalesWhereProfitNegativeAndYearAf...</td>\n      <td>filtration, join</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>filterCatalogSalesWhereYearAfter2000</td>\n      <td>filtration, join</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>filterStoreSalesWhereProfitNegative</td>\n      <td>filtration</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>filterStoreSalesWhereProfitNegativeAndYearAfte...</td>\n      <td>filtration, join</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>filterStoreSalesWhereYearAfter2000</td>\n      <td>filtration, join</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>maxNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>maxNetProfitGroupedBySoldDateWhereProfitNegative</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>maxNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>maxWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>minNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>minNetProfitGroupedBySoldDateWhereProfitNegative</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>minNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>minWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>sumNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>sumNetProfitGroupedBySoldDateWhereProfitNegative</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>sumNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>sumWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>summaryNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>summaryWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"./../notebooks/functions.csv\")\n",
    "labels = labels.groupby('function_name')['label'].apply(', '.join).reset_index()\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grouping labels by UDF type\n",
    "Plot colors:\n",
    "    * aggregation - blue,\n",
    "    * filtration - red,\n",
    "    * aggregation-filtration - green,\n",
    "    * aggregation-filtration-join - purple."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg: 12\n",
      "fil: 2\n",
      "agg-fil: 5\n",
      "filtration-join: 4\n",
      "agg-fil-join: 5\n",
      "Max: 28 | Suma: 28\n"
     ]
    }
   ],
   "source": [
    "agg_labeled = labels.loc[labels[\"label\"] == \"aggregation\"]\n",
    "agg_fil_labeled = labels.loc[labels[\"label\"] == \"aggregation, filtration\"]\n",
    "fil_labeled = labels.loc[labels[\"label\"] == \"filtration\"]\n",
    "fil_join_labeled = labels.loc[labels[\"label\"] == \"filtration, join\"]\n",
    "agg_fil_join_labeled = labels.loc[labels[\"label\"] == \"aggregation, filtration, join\"]\n",
    "\n",
    "print(f\"agg: {len(agg_labeled)}\")\n",
    "print(f\"fil: {len(fil_labeled)}\")\n",
    "print(f\"agg-fil: {len(agg_fil_labeled)}\")\n",
    "print(f\"filtration-join: {len(fil_join_labeled)}\")\n",
    "print(f\"agg-fil-join: {len(agg_fil_join_labeled)}\")\n",
    "print(f\"Max: {len(function_names)} | Suma: {len(agg_labeled)+len(fil_labeled)+len(agg_fil_labeled)+len(agg_fil_join_labeled)+len(fil_join_labeled)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Function for translating (move mean to zero) and scaling (multiply by reverse standard deviation) original data (mean from worker nodes)\n",
    "def translate_scale(dataframe):\n",
    "    dataframe[\"translated\"] = dataframe['CPU'] - dataframe['CPU'].mean()\n",
    "    dataframe[\"scaled\"] = dataframe[\"translated\"] * (1/dataframe['CPU'].std())\n",
    "    return dataframe\n",
    "\n",
    "def dtw_distance(x_labels, y_labels):\n",
    "    distances = []\n",
    "    for x_name in x_labels[\"function_name\"]:\n",
    "        try:\n",
    "            x_data = pd.read_csv(f\"./../{working_directory}/preprocessed-data/workers-mean-data/{x_name}/translated_scaled_smoothed_data.csv\")\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "        for y_name in y_labels[\"function_name\"]:\n",
    "            try:\n",
    "                y_data = pd.read_csv(f\"./../{working_directory}/preprocessed-data/workers-mean-data/{y_name}/translated_scaled_smoothed_data.csv\")\n",
    "                distance_two_sec = dtw.distance_fast(x_data[\"two_sec\"].to_numpy(dtype=np.double), y_data[\"two_sec\"].to_numpy(dtype=np.double))\n",
    "                distances.append(distance_two_sec)\n",
    "            except OSError as e:\n",
    "                print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    distances.sort()\n",
    "    distances = [i for i in distances if i != 0.0]\n",
    "    return round(min(distances), 2), round(max(distances), 2), round(mean(distances), 2), round(pstdev(distances), 2), distances,\n",
    "\n",
    "def group_distance_statistics(labels_X, labels_Y, name_X, name_Y, dataframe):\n",
    "    min_value, max_value, mean_value, std_value , all_distances = dtw_distance(labels_X, labels_Y)\n",
    "    dataframe = dataframe.append({\"name\": f\"X:{name_X} Y:{name_Y}\",\n",
    "                                  \"min\": min_value,\n",
    "                                  \"max\": max_value,\n",
    "                                  \"mean\": mean_value,\n",
    "                                  \"std\": std_value,\n",
    "                                  \"distances\": all_distances}, ignore_index=True)\n",
    "    print(\"\\n\")\n",
    "    print(\"----------------------\")\n",
    "    print(f\"{name_X} - {name_Y}\")\n",
    "    print(f\"MIN: {min_value}\")\n",
    "    print(f\"MAX: {max_value}\")\n",
    "    print(f\"MEAN: {mean_value}\")\n",
    "    print(f\"STD: {std_value}\")\n",
    "    return dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transforming data for Dynamic Time Warping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IN dataframe: \"CPU\"\n",
    "# OUT dataframe: \"translated\", \"scaled\", \"one_sec\", \"two_sec\", \"five_sec\", \"ten_sec\"\n",
    "\n",
    "for name in function_names:\n",
    "    file_path = f\"./../{working_directory}/preprocessed-data/workers-mean-data{name}\"\n",
    "    original_data = pd.read_csv(f\"{file_path}/mean_data.csv\")\n",
    "    original_data = original_data[[\"CPU\"]]\n",
    "    transformed_data = translate_scale(original_data)\n",
    "    transformed_data[\"one_sec\"] = transformed_data[\"scaled\"].rolling(4, min_periods=1, center=True).mean()\n",
    "    transformed_data[\"two_sec\"] = transformed_data[\"scaled\"].rolling(8, min_periods=1, center=True).mean()\n",
    "    transformed_data[\"two_half_sec\"] = transformed_data[\"scaled\"].rolling(10, min_periods=1, center=True).mean()\n",
    "    transformed_data[\"five_sec\"] = transformed_data[\"scaled\"].rolling(20, min_periods=1, center=True).mean()\n",
    "    transformed_data[\"ten_sec\"] = transformed_data[\"scaled\"].rolling(40, min_periods=1, center=True).mean()\n",
    "    transformed_data.to_csv(f\"{file_path}/translated_scaled_smoothed_data.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-49-fd6fa2517e72>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mdel\u001B[0m \u001B[0mtransformed_data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mdel\u001B[0m \u001B[0mcomparison_statistics\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'transformed_data' is not defined"
     ]
    }
   ],
   "source": [
    "del transformed_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# # Comparing udfs round-robin"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------\n",
      "aggregation - aggregation\n",
      "MIN: 1.6\n",
      "MAX: 7.02\n",
      "MEAN: 3.36\n",
      "STD: 1.13\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation - filtration\n",
      "MIN: 20.43\n",
      "MAX: 27.0\n",
      "MEAN: 23.34\n",
      "STD: 2.24\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration - filtration\n",
      "MIN: 2.71\n",
      "MAX: 2.71\n",
      "MEAN: 2.71\n",
      "STD: 0.0\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation - aggregation-filtration\n",
      "MIN: 3.16\n",
      "MAX: 7.04\n",
      "MEAN: 4.55\n",
      "STD: 0.96\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration - aggregation-filtration\n",
      "MIN: 16.07\n",
      "MAX: 20.81\n",
      "MEAN: 18.55\n",
      "STD: 2.02\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation-filtration - aggregation-filtration\n",
      "MIN: 1.68\n",
      "MAX: 4.29\n",
      "MEAN: 2.83\n",
      "STD: 0.89\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation - aggregation-filtration-join\n",
      "MIN: 3.74\n",
      "MAX: 7.83\n",
      "MEAN: 5.0\n",
      "STD: 0.96\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration - aggregation-filtration-join\n",
      "MIN: 15.76\n",
      "MAX: 21.14\n",
      "MEAN: 18.39\n",
      "STD: 2.18\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration-join - aggregation\n",
      "MIN: 19.47\n",
      "MAX: 28.28\n",
      "MEAN: 23.57\n",
      "STD: 2.36\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration-join - filtration\n",
      "MIN: 2.21\n",
      "MAX: 4.54\n",
      "MEAN: 3.22\n",
      "STD: 0.83\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration-join - aggregation-filtration\n",
      "MIN: 14.92\n",
      "MAX: 21.74\n",
      "MEAN: 18.58\n",
      "STD: 2.23\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration-join - filtration-join\n",
      "MIN: 1.24\n",
      "MAX: 4.16\n",
      "MEAN: 2.37\n",
      "STD: 0.99\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration-join - aggregation-filtration-join\n",
      "MIN: 15.18\n",
      "MAX: 22.31\n",
      "MEAN: 18.64\n",
      "STD: 2.24\n"
     ]
    }
   ],
   "source": [
    "comparison_statistics = pd.DataFrame(columns=[\"name\", \"min\", \"max\", \"mean\", \"std\", \"distances\"])\n",
    "comparison_statistics = group_distance_statistics(agg_labeled, agg_labeled, \"aggregation\", \"aggregation\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_labeled, fil_labeled, \"aggregation\", \"filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_labeled, fil_labeled, \"filtration\", \"filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_labeled, agg_fil_labeled, \"aggregation\", \"aggregation-filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_labeled, agg_fil_labeled, \"filtration\", \"aggregation-filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_fil_labeled, agg_fil_labeled, \"aggregation-filtration\", \"aggregation-filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_labeled, agg_fil_join_labeled, \"aggregation\", \"aggregation-filtration-join\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_labeled, agg_fil_join_labeled, \"filtration\", \"aggregation-filtration-join\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_join_labeled, agg_labeled, \"filtration-join\", \"aggregation\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_join_labeled, fil_labeled, \"filtration-join\", \"filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_join_labeled, agg_fil_labeled, \"filtration-join\", \"aggregation-filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_join_labeled, fil_join_labeled, \"filtration-join\", \"filtration-join\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_join_labeled, agg_fil_join_labeled, \"filtration-join\", \"aggregation-filtration-join\", comparison_statistics)\n",
    "\n",
    "# # Saving the data  with comparison statistics to file\n",
    "comparison_statistics.to_csv(f\"./../{working_directory}/preprocessed-data/workers-mean-data/dtw_dtai_statistics.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating dtw statistics box plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "box_plot = pygal.Box(truncate_legend = 25, width = 1000, height = 1000, legend_at_bottom = True, logarithmic=True)\n",
    "box_plot.title = f\"Function comparison statistics {working_directory}odes\"\n",
    "for iteration in range(comparison_statistics.shape[0]):\n",
    "    box_plot.add(comparison_statistics.iloc[iteration,0], comparison_statistics.iloc[iteration,5])\n",
    "box_plot.render_to_file(f\"./../{working_directory}/preprocessed-data/workers-mean-data/dtai-udf-round-robin-comparison.svg\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "del comparison_statistics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating plots for every function\n",
    "    * original - red,\n",
    "    * transformed - green,\n",
    "    * scaled - magenta,\n",
    "    * 1 sec window - yellow,\n",
    "    * 2 sec window - cyan,\n",
    "    * 5 sec window - black,\n",
    "    * 10 sec window - blue."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/avgNetProfitGroupedBySoldDate/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/avgNetProfitGroupedBySoldDateWhereProfitNegative/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/avgNetProfitGroupedBySoldDateWhereYearAfter2000/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/avgWholeSaleCostGroupedBySoldDate/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/countNetProfitGroupedBySoldDate/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/countNetProfitGroupedBySoldDateWhereProfitNegative/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/countNetProfitGroupedBySoldDateWhereYearAfter2000/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/countWholeSaleCostGroupedBySoldDate/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/filterCatalogSalesWhereProfitNegative/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/filterCatalogSalesWhereProfitNegativeAndYearAfter2000/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/filterCatalogSalesWhereYearAfter2000/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/filterStoreSalesWhereProfitNegative/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/filterStoreSalesWhereProfitNegativeAndYearAfter2000/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/filterStoreSalesWhereYearAfter2000/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/maxNetProfitGroupedBySoldDate/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/maxNetProfitGroupedBySoldDateWhereProfitNegative/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/maxNetProfitGroupedBySoldDateWhereYearAfter2000/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/maxWholeSaleCostGroupedBySoldDate/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/minNetProfitGroupedBySoldDate/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/minNetProfitGroupedBySoldDateWhereProfitNegative/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/minNetProfitGroupedBySoldDateWhereYearAfter2000/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/minWholeSaleCostGroupedBySoldDate/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/summaryNetProfitGroupedBySoldDate/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/summaryWholeSaleCostGroupedBySoldDate/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/sumNetProfitGroupedBySoldDate/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/sumNetProfitGroupedBySoldDateWhereProfitNegative/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/sumNetProfitGroupedBySoldDateWhereYearAfter2000/dtw - Nie można utworzyć pliku, który już istnieje.\n",
      "Error: ./../2GB-9N/preprocessed-data/workers-mean-data/sumWholeSaleCostGroupedBySoldDate/dtw - Nie można utworzyć pliku, który już istnieje.\n"
     ]
    }
   ],
   "source": [
    "for name in function_names:\n",
    "\n",
    "    file_path = f\"./../{working_directory}/preprocessed-data/workers-mean-data{name}\"\n",
    "    figures_path = f\"{file_path}/dtw\"\n",
    "    try:\n",
    "        os.mkdir(figures_path)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    read_data = pd.read_csv(f\"{file_path}/translated_scaled_smoothed_data.csv\")\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"CPU\"], 'r-', label=\"original\")\n",
    "    plt.plot(read_data[\"translated\"], 'g-', label=\"translated\")\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: original, translated, scaled\", fontsize=15)\n",
    "    plt.xlabel(\"Numer próbki\", fontsize=15)\n",
    "    plt.ylabel(\"Średnie procentowe zużycie CPU\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/org_trans_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"one_sec\"], 'k-', label=\"1s window\")\n",
    "    # plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 1s)\", fontsize=15)\n",
    "    plt.xlabel(\"Numer próbki\", fontsize=10)\n",
    "    plt.ylabel(\"Średnie procentowe zużycie CPU\", fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/1_sec_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"two_sec\"], 'k-', label=\"2s window\")\n",
    "    # plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 2s)\", fontsize=15)\n",
    "    plt.xlabel(\"Numer próbki\", fontsize=10)\n",
    "    plt.ylabel(\"Średnie procentowe zużycie CPU\", fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/2_sec_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"two_half_sec\"], 'k-', label=\"2.5s window\")\n",
    "    # plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 2.5s)\", fontsize=15)\n",
    "    plt.xlabel(\"Numer próbki\", fontsize=10)\n",
    "    plt.ylabel(\"Średnie procentowe zużycie CPU\", fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/2_5_sec_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"five_sec\"], 'k-', label=\"5s window\")\n",
    "    # plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 5s)\", fontsize=15)\n",
    "    plt.xlabel(\"Numer próbki\", fontsize=10)\n",
    "    plt.ylabel(\"Średnie procentowe zużycie CPU\", fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/5_sec_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"ten_sec\"], 'k-', label=\"10s window\")\n",
    "    # plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 10s)\", fontsize=15)\n",
    "    plt.xlabel(\"Numer próbki\", fontsize=10)\n",
    "    plt.ylabel(\"Średnie procentowe zużycie CPU\", fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/10_sec_scaled.png\")\n",
    "    plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions of the same type in one plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def plot_udf_type(udf_type, udf_type_name):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    for name in udf_type[\"function_name\"]:\n",
    "        try:\n",
    "            file_path = f\"./../{working_directory}/preprocessed-data/workers-mean-data/{name}\"\n",
    "            read_data = pd.read_csv(f\"{file_path}/translated_scaled_smoothed_data.csv\")\n",
    "            print(f\"{name}: done\")\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "        plt.plot(read_data[\"five_sec\"], '-', label=name)\n",
    "    plt.title(f\" 5s window | {udf_type_name}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./../{working_directory}/preprocessed-data/workers-mean-data/{udf_type_name}.png\")\n",
    "    plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgNetProfitGroupedBySoldDate: done\n",
      "avgWholeSaleCostGroupedBySoldDate: done\n",
      "countNetProfitGroupedBySoldDate: done\n",
      "countWholeSaleCostGroupedBySoldDate: done\n",
      "maxNetProfitGroupedBySoldDate: done\n",
      "maxWholeSaleCostGroupedBySoldDate: done\n",
      "minNetProfitGroupedBySoldDate: done\n",
      "minWholeSaleCostGroupedBySoldDate: done\n",
      "sumNetProfitGroupedBySoldDate: done\n",
      "sumWholeSaleCostGroupedBySoldDate: done\n",
      "summaryNetProfitGroupedBySoldDate: done\n",
      "summaryWholeSaleCostGroupedBySoldDate: done\n",
      "filterCatalogSalesWhereProfitNegative: done\n",
      "filterStoreSalesWhereProfitNegative: done\n",
      "avgNetProfitGroupedBySoldDateWhereProfitNegative: done\n",
      "countNetProfitGroupedBySoldDateWhereProfitNegative: done\n",
      "maxNetProfitGroupedBySoldDateWhereProfitNegative: done\n",
      "minNetProfitGroupedBySoldDateWhereProfitNegative: done\n",
      "sumNetProfitGroupedBySoldDateWhereProfitNegative: done\n",
      "filterCatalogSalesWhereProfitNegativeAndYearAfter2000: done\n",
      "filterCatalogSalesWhereYearAfter2000: done\n",
      "filterStoreSalesWhereProfitNegativeAndYearAfter2000: done\n",
      "filterStoreSalesWhereYearAfter2000: done\n",
      "avgNetProfitGroupedBySoldDateWhereYearAfter2000: done\n",
      "countNetProfitGroupedBySoldDateWhereYearAfter2000: done\n",
      "maxNetProfitGroupedBySoldDateWhereYearAfter2000: done\n",
      "minNetProfitGroupedBySoldDateWhereYearAfter2000: done\n",
      "sumNetProfitGroupedBySoldDateWhereYearAfter2000: done\n"
     ]
    }
   ],
   "source": [
    "plot_udf_type(agg_labeled, \"aggregation\")\n",
    "plot_udf_type(fil_labeled, \"filtration\")\n",
    "plot_udf_type(agg_fil_labeled, \"aggregation-filtration\")\n",
    "plot_udf_type(fil_join_labeled, \"filtration-join\")\n",
    "plot_udf_type(agg_fil_join_labeled, \"aggregation-filtration-join\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparing data from 1GB and 2GB dataset sizes\n",
    "Creating `udf-vs-datasetsize` folder in working directory\n",
    "\n",
    "Creating subfolders for each function type (`agg`, `fil`, `agg-fil`, `fil-join`, `agg-fil-join`)\n",
    "\n",
    "Gathering data about function length (number of samples) in `function_types_duration.csv`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ./../udf-vs-datasetsize - Nie można utworzyć pliku, który już istnieje.\n"
     ]
    }
   ],
   "source": [
    "figures_path = f\"./../udf-vs-datasetsize\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(figures_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function: avgNetProfitGroupedBySoldDate - done\n",
      "function: avgWholeSaleCostGroupedBySoldDate - done\n",
      "function: countNetProfitGroupedBySoldDate - done\n",
      "function: countWholeSaleCostGroupedBySoldDate - done\n",
      "function: maxNetProfitGroupedBySoldDate - done\n",
      "function: maxWholeSaleCostGroupedBySoldDate - done\n",
      "function: minNetProfitGroupedBySoldDate - done\n",
      "function: minWholeSaleCostGroupedBySoldDate - done\n",
      "function: sumNetProfitGroupedBySoldDate - done\n",
      "function: sumWholeSaleCostGroupedBySoldDate - done\n",
      "function: summaryNetProfitGroupedBySoldDate - done\n",
      "function: summaryWholeSaleCostGroupedBySoldDate - done\n",
      "function: filterCatalogSalesWhereProfitNegative - done\n",
      "function: filterStoreSalesWhereProfitNegative - done\n",
      "function: avgNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "function: countNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "function: maxNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "function: minNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "function: sumNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "function: filterCatalogSalesWhereProfitNegativeAndYearAfter2000 - done\n",
      "function: filterCatalogSalesWhereYearAfter2000 - done\n",
      "function: filterStoreSalesWhereProfitNegativeAndYearAfter2000 - done\n",
      "function: filterStoreSalesWhereYearAfter2000 - done\n",
      "function: avgNetProfitGroupedBySoldDateWhereYearAfter2000 - done\n",
      "function: countNetProfitGroupedBySoldDateWhereYearAfter2000 - done\n",
      "function: maxNetProfitGroupedBySoldDateWhereYearAfter2000 - done\n",
      "function: minNetProfitGroupedBySoldDateWhereYearAfter2000 - done\n",
      "function: sumNetProfitGroupedBySoldDateWhereYearAfter2000 - done\n"
     ]
    }
   ],
   "source": [
    "function_type_duration = pd.DataFrame(columns=[\"function_type\",\n",
    "                                             \"size-config\",\n",
    "                                             \"length\"])\n",
    "\n",
    "dtw_vs_dataset_size = pd.DataFrame(columns=[\"function_name\", \"distance\"])\n",
    "\n",
    "function_types = [agg_labeled[\"function_name\"],\n",
    "                  fil_labeled[\"function_name\"],\n",
    "                  agg_fil_labeled[\"function_name\"],\n",
    "                  fil_join_labeled[\"function_name\"],\n",
    "                  agg_fil_join_labeled[\"function_name\"]]\n",
    "\n",
    "function_directory_names = [\"aggregation\",\n",
    "                            \"filtration\",\n",
    "                            \"agg-filtration\",\n",
    "                            \"fil-join\",\n",
    "                            \"agg-filtration-join\"]\n",
    "for index in range (0,5):\n",
    "    for name in function_types[index]:\n",
    "        tmp_path = f\"{figures_path}/{function_directory_names[index]}\"\n",
    "        print(f\"function: {name} - done\")\n",
    "        try:\n",
    "            os.mkdir(tmp_path)\n",
    "        except OSError as e:\n",
    "            # print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "            x = 1\n",
    "        for nodes in [9]:\n",
    "\n",
    "            one_gb_data = pd.read_csv(f\"./../1GB-{nodes}N/preprocessed-data/workers-mean-data/{name}/translated_scaled_smoothed_data.csv\")\n",
    "            two_gb_data = pd.read_csv(f\"./../2GB-{nodes}N/preprocessed-data/workers-mean-data/{name}/translated_scaled_smoothed_data.csv\")\n",
    "\n",
    "            # Measuring function type duration (A) - if (B) comment this part\n",
    "            function_type_duration = function_type_duration.append({\"function_type\": function_directory_names[index],\n",
    "                                                                    \"size-config\": f\"1GB-{nodes}N\",\n",
    "                                                                    \"length\":  int(one_gb_data['two_sec'].shape[0])}, ignore_index=True)\n",
    "\n",
    "            function_type_duration = function_type_duration.append({\"function_type\": function_directory_names[index],\n",
    "                                                                    \"size-config\": f\"2GB-{nodes}N\",\n",
    "                                                                    \"length\":  int(two_gb_data['two_sec'].shape[0])}, ignore_index=True)\n",
    "\n",
    "            # Measuring distance between 1GB and 2GB functions (B) - if (A) comment this part\n",
    "            # dtw_vs_dataset_size = dtw_vs_dataset_size.append({\"function_name\": name,\n",
    "            #                                                   \"distance\": round(dtw.distance_fast(one_gb_data[\"two_sec\"].to_numpy(dtype=np.double),\n",
    "            #                                                                                       two_gb_data[\"two_sec\"].to_numpy(dtype=np.double),\n",
    "            #                                                                                       window=20,\n",
    "            #                                                                                       use_pruning=True), 2)}, ignore_index=True)\n",
    "\n",
    "            # (A) - if (B) comment this part\n",
    "            fig = plt.figure(figsize=(20,10))\n",
    "            fig.patch.set_facecolor('white')\n",
    "            plt.plot(one_gb_data[\"two_sec\"], 'k-', label=f\"1 GB | len:{one_gb_data['two_sec'].shape[0]}\")\n",
    "            plt.plot(two_gb_data[\"two_sec\"], '-', label=f\"2 GB | len:{two_gb_data['two_sec'].shape[0]}\", color=\"olive\")\n",
    "            plt.xlabel(\"Numer próbki\", fontsize=15)\n",
    "            plt.ylabel(\"Średnie procentowe zużycie CPU\", fontsize=15)\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"{tmp_path}/{name}_1GBvs2GB-{nodes}N.png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "# (A) - if (B) comment this part\n",
    "function_type_duration.to_csv(f\"{figures_path}/function_types_length.csv\", index=False)\n",
    "\n",
    "# (B) - if (A) comment this part\n",
    "# dtw_vs_dataset_size.to_csv(f\"{figures_path}/dtw-vs-dataset-size.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Changing length column to `numeric`\n",
    "\n",
    "Aggregating data by function type and size-configuration\n",
    "\n",
    "Gathering mean and std\n",
    "\n",
    "Saving aggregated data to `mean-function-types-length.csv`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "function_type_duration[\"length\"] = pd.to_numeric(function_type_duration[\"length\"])\n",
    "\n",
    "function_type_mean = function_type_duration.groupby(['function_type', 'size-config']).agg(mean_length=(\"length\", \"mean\"),\n",
    "                                                                                          std_length=(\"length\", \"std\"))\n",
    "function_type_mean = function_type_mean.round(2)\n",
    "\n",
    "function_type_mean.to_csv(f\"{figures_path}/mean-function-types-length.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "for nodes in [5, 7, 9]:\n",
    "    mean_nodes_length = pd.read_csv(f\"{figures_path}/mean-function-types-length.csv\")\n",
    "    mean_nodes_length = mean_nodes_length[(mean_nodes_length[\"size-config\"] == f\"1GB-{nodes}N\") | (mean_nodes_length[\"size-config\"] == f\"2GB-{nodes}N\")]\n",
    "    mean_nodes_length.to_csv(f\"{figures_path}/{nodes}N_mean_length.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UDF vs cluster configuration 1GB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ./../udf-vs-cluster-config - Nie można utworzyć pliku, który już istnieje.\n"
     ]
    }
   ],
   "source": [
    "figures_path = f\"./../udf-vs-cluster-config\"\n",
    "try:\n",
    "    os.mkdir(figures_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "figures_path_vs_cluster = f\"./../udf-vs-cluster-config/2GB\"\n",
    "try:\n",
    "    os.mkdir(figures_path_vs_cluster)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "function_type_duration_cluster = pd.DataFrame(columns=[\"function_type\",\n",
    "                                                       \"size-config\",\n",
    "                                                       \"length\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function: avgNetProfitGroupedBySoldDate - done\n",
      "function: avgWholeSaleCostGroupedBySoldDate - done\n",
      "function: countNetProfitGroupedBySoldDate - done\n",
      "function: countWholeSaleCostGroupedBySoldDate - done\n",
      "function: maxNetProfitGroupedBySoldDate - done\n",
      "function: maxWholeSaleCostGroupedBySoldDate - done\n",
      "function: minNetProfitGroupedBySoldDate - done\n",
      "function: minWholeSaleCostGroupedBySoldDate - done\n",
      "function: sumNetProfitGroupedBySoldDate - done\n",
      "function: sumWholeSaleCostGroupedBySoldDate - done\n",
      "function: summaryNetProfitGroupedBySoldDate - done\n",
      "function: summaryWholeSaleCostGroupedBySoldDate - done\n",
      "function: filterCatalogSalesWhereProfitNegative - done\n",
      "function: filterStoreSalesWhereProfitNegative - done\n",
      "function: avgNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "function: countNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "function: maxNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "function: minNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "function: sumNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "function: filterCatalogSalesWhereProfitNegativeAndYearAfter2000 - done\n",
      "function: filterCatalogSalesWhereYearAfter2000 - done\n",
      "function: filterStoreSalesWhereProfitNegativeAndYearAfter2000 - done\n",
      "function: filterStoreSalesWhereYearAfter2000 - done\n",
      "function: avgNetProfitGroupedBySoldDateWhereYearAfter2000 - done\n",
      "function: countNetProfitGroupedBySoldDateWhereYearAfter2000 - done\n",
      "function: maxNetProfitGroupedBySoldDateWhereYearAfter2000 - done\n",
      "function: minNetProfitGroupedBySoldDateWhereYearAfter2000 - done\n",
      "function: sumNetProfitGroupedBySoldDateWhereYearAfter2000 - done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hubert\\AppData\\Local\\Temp/ipykernel_3940/3891489307.py:28: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k-\" (-> color='k'). The keyword argument will take precedence.\n",
      "  plt.plot(five_nodes_data[\"two_sec\"], 'k-', label=f\"5 węzłów | dł.:{five_nodes_data['two_sec'].shape[0]}\", color=\"red\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index in range (0,5):\n",
    "    for name in function_types[index]:\n",
    "        five_nodes_data = pd.read_csv(f\"./../2GB-5N/preprocessed-data/workers-mean-data/{name}/translated_scaled_smoothed_data.csv\")\n",
    "        seven_nodes_data = pd.read_csv(f\"./../2GB-7N/preprocessed-data/workers-mean-data/{name}/translated_scaled_smoothed_data.csv\")\n",
    "        nine_nodes_data = pd.read_csv(f\"./../2GB-9N/preprocessed-data/workers-mean-data/{name}/translated_scaled_smoothed_data.csv\")\n",
    "\n",
    "        tmp_path = f\"{figures_path_vs_cluster}/{function_directory_names[index]}\"\n",
    "        print(f\"function: {name} - done\")\n",
    "        try:\n",
    "            os.mkdir(tmp_path)\n",
    "        except OSError as e:\n",
    "            # print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "            x = 1\n",
    "        function_type_duration_cluster = function_type_duration_cluster.append({\"function_type\": function_directory_names[index],\n",
    "                                                                    \"size-config\": f\"2GB-5N\",\n",
    "                                                                    \"length\":  int(five_nodes_data['two_sec'].shape[0])}, ignore_index=True)\n",
    "\n",
    "        function_type_duration_cluster = function_type_duration_cluster.append({\"function_type\": function_directory_names[index],\n",
    "                                                                    \"size-config\": f\"2GB-7N\",\n",
    "                                                                    \"length\":  int(seven_nodes_data['two_sec'].shape[0])}, ignore_index=True)\n",
    "\n",
    "        function_type_duration_cluster = function_type_duration_cluster.append({\"function_type\": function_directory_names[index],\n",
    "                                                                    \"size-config\": f\"2GB-9N\",\n",
    "                                                                    \"length\":  int(nine_nodes_data['two_sec'].shape[0])}, ignore_index=True)\n",
    "\n",
    "        fig = plt.figure(figsize=(20,10))\n",
    "        fig.patch.set_facecolor('white')\n",
    "        plt.plot(five_nodes_data[\"two_sec\"], 'k-', label=f\"5 węzłów | dł.:{five_nodes_data['two_sec'].shape[0]}\", color=\"red\")\n",
    "        plt.plot(seven_nodes_data[\"two_sec\"], '-', label=f\"7 węzłów | dł.:{seven_nodes_data['two_sec'].shape[0]}\", color=\"olive\")\n",
    "        plt.plot(nine_nodes_data[\"two_sec\"], '-', label=f\"9 węzłów | dł.:{nine_nodes_data['two_sec'].shape[0]}\", color=\"blue\")\n",
    "        # plt.title(f\"Function: {name[1:]} | 1GB | Nodes: 5, 7, 9\", fontsize=15)\n",
    "        plt.xlabel(\"Numer próbki\", fontsize=15)\n",
    "        plt.ylabel(\"Średnie procentowe zużycie CPU\", fontsize=15)\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{tmp_path}/{name}.png\")\n",
    "        plt.close(fig)\n",
    "function_type_duration_cluster.to_csv(f\"{figures_path}/function_types_length.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "function_type_duration_cluster[\"length\"] = pd.to_numeric(function_type_duration_cluster[\"length\"])\n",
    "\n",
    "function_type_mean_cluster = function_type_duration_cluster.groupby(['function_type', 'size-config']).agg(mean_length=(\"length\", \"mean\"),\n",
    "                                                                                                          std_length=(\"length\", \"std\"))\n",
    "function_type_mean_cluster = function_type_mean_cluster.round(2)\n",
    "\n",
    "function_type_mean_cluster.to_csv(f\"{figures_path}/mean-function-types-length.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean up"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# Deleting created files\n",
    "\n",
    "for function_name in function_names:\n",
    "    try:\n",
    "        os.remove(f\"./../{working_directory}/preprocessed-data/workers-mean-data{function_name}/translated_scaled_smoothed_data.csv\")\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "for function_name in function_names:\n",
    "    try:\n",
    "        shutil.rmtree(f\"./../{working_directory}/preprocessed-data/workers-mean-data{function_name}/dtw\")\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparing different DTW packages with agg functions `avgNetProfitGroupedBySoldDate` and `avgWholeSaleCostGroupedBySoldDate`.\n",
    "* dtai\n",
    "* fastdtw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "dtw_comparison = pd.DataFrame(columns=[\"type\",\n",
    "                                       \"distances\",\n",
    "                                       \"durations\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series one length: 1732\n",
      "Series two length: 1740\n"
     ]
    }
   ],
   "source": [
    "series_one = pd.read_csv(f\"./../2GB-9N/preprocessed-data/workers-mean-data/{agg_labeled['function_name'][0]}/translated_scaled_smoothed_data.csv\")\n",
    "series_two = pd.read_csv(f\"./../2GB-9N/preprocessed-data/workers-mean-data/{agg_labeled['function_name'][3]}/translated_scaled_smoothed_data.csv\")\n",
    "\n",
    "print(f\"Series one length: {series_one.shape[0]}\")\n",
    "print(f\"Series two length: {series_two.shape[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dtai"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28.380415800000264\n",
      "1 27.66166020000128\n",
      "2 29.415623900000355\n",
      "3 26.85681730000033\n",
      "4 27.40669369999887\n",
      "5 26.80497739999919\n",
      "6 26.71533719999934\n",
      "7 26.358528300001126\n",
      "8 29.553704500000094\n",
      "9 26.585896300000968\n",
      "10 26.175347300000794\n",
      "11 25.981407799999943\n",
      "12 25.64267860000109\n",
      "13 25.80749999999898\n",
      "14 26.07061860000067\n",
      "15 26.379649500000596\n",
      "16 25.69651089999934\n",
      "17 25.86894620000021\n",
      "18 25.629505599999902\n",
      "19 25.86495079999986\n",
      "20 31.54584309999882\n",
      "21 31.934932400001344\n",
      "22 28.926208699998824\n",
      "23 27.802554199999577\n",
      "24 29.137903200000437\n",
      "25 31.171985700000732\n",
      "26 34.10939320000034\n",
      "27 30.564726900000096\n",
      "28 32.20590909999919\n",
      "29 26.544843499999843\n",
      "[2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403]\n",
      "[28.380415800000264, 27.66166020000128, 29.415623900000355, 26.85681730000033, 27.40669369999887, 26.80497739999919, 26.71533719999934, 26.358528300001126, 29.553704500000094, 26.585896300000968, 26.175347300000794, 25.981407799999943, 25.64267860000109, 25.80749999999898, 26.07061860000067, 26.379649500000596, 25.69651089999934, 25.86894620000021, 25.629505599999902, 25.86495079999986, 31.54584309999882, 31.934932400001344, 28.926208699998824, 27.802554199999577, 29.137903200000437, 31.171985700000732, 34.10939320000034, 30.564726900000096, 32.20590909999919, 26.544843499999843]\n"
     ]
    }
   ],
   "source": [
    "dtai_durations = []\n",
    "dtai_distances = []\n",
    "for iteration in range(30):\n",
    "    start_dtai = timer()\n",
    "    dtai_distance = dtw.distance(series_one[\"two_sec\"], series_two[\"two_sec\"])\n",
    "    end_dtai = timer()\n",
    "    print(f\"{iteration} {end_dtai-start_dtai}\")\n",
    "    dtai_durations.append(end_dtai-start_dtai)\n",
    "    dtai_distances.append(dtai_distance)\n",
    "print(dtai_distances)\n",
    "print(dtai_durations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "dtw_comparison  = dtw_comparison.append({\"type\": \"dtai\",\n",
    "                                         \"distances\": dtai_distances,\n",
    "                                         \"durations\": dtai_durations},\n",
    "                                        ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dtai_fast - c implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.025364900000568014\n",
      "1 0.026127099999939674\n",
      "2 0.02418500000021595\n",
      "3 0.02072790000056557\n",
      "4 0.020152700000835466\n",
      "5 0.019802599999820814\n",
      "6 0.019811199999821838\n",
      "7 0.019055200000366312\n",
      "8 0.01955459999953746\n",
      "9 0.019209899999623303\n",
      "10 0.019991500001196982\n",
      "11 0.018452299998898525\n",
      "12 0.019202800000130082\n",
      "13 0.024634000001242384\n",
      "14 0.018911400000433787\n",
      "15 0.019523500001014327\n",
      "16 0.022711199999321252\n",
      "17 0.020310699999754434\n",
      "18 0.020627899999453803\n",
      "19 0.42328290000114066\n",
      "20 0.02370580000024347\n",
      "21 0.0189782000015839\n",
      "22 0.01901209999959974\n",
      "23 0.019382000000405242\n",
      "24 0.0195785000014439\n",
      "25 0.022003900001436705\n",
      "26 0.018768899999486166\n",
      "27 0.01988589999928081\n",
      "28 0.018954799999846728\n",
      "29 0.018935400001282687\n",
      "[2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403, 2.6727449869192403]\n",
      "[0.025364900000568014, 0.026127099999939674, 0.02418500000021595, 0.02072790000056557, 0.020152700000835466, 0.019802599999820814, 0.019811199999821838, 0.019055200000366312, 0.01955459999953746, 0.019209899999623303, 0.019991500001196982, 0.018452299998898525, 0.019202800000130082, 0.024634000001242384, 0.018911400000433787, 0.019523500001014327, 0.022711199999321252, 0.020310699999754434, 0.020627899999453803, 0.42328290000114066, 0.02370580000024347, 0.0189782000015839, 0.01901209999959974, 0.019382000000405242, 0.0195785000014439, 0.022003900001436705, 0.018768899999486166, 0.01988589999928081, 0.018954799999846728, 0.018935400001282687]\n"
     ]
    }
   ],
   "source": [
    "dtai_f_durations = []\n",
    "dtai_f_distances = []\n",
    "for iteration in range(30):\n",
    "    start_dtai_f = timer()\n",
    "    dtai_f_distance = dtw.distance_fast(series_one[\"two_sec\"].to_numpy(dtype=np.double), series_two[\"two_sec\"].to_numpy(dtype=np.double))\n",
    "    end_dtai_f = timer()\n",
    "    print(f\"{iteration} {end_dtai_f-start_dtai_f}\")\n",
    "    dtai_f_durations.append(end_dtai_f-start_dtai_f)\n",
    "    dtai_f_distances.append(dtai_f_distance)\n",
    "print(dtai_f_distances)\n",
    "print(dtai_f_durations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "dtw_comparison  = dtw_comparison.append({\"type\": \"dtai_fast\",\n",
    "                                         \"distances\": dtai_f_distances,\n",
    "                                         \"durations\": dtai_f_durations},\n",
    "                                        ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "FastDTW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.7950084000003699\n",
      "1 1.1660730999992666\n",
      "2 1.057933199999752\n",
      "3 1.0520505999993475\n",
      "4 1.095794200000455\n",
      "5 1.0888614999985293\n",
      "6 1.2580463999984204\n",
      "7 1.0814057999996294\n",
      "8 1.0809062000007543\n",
      "9 1.0772259999994276\n",
      "10 1.0882581000005302\n",
      "11 1.078080499999487\n",
      "12 1.05712019999919\n",
      "13 1.0507240999995702\n",
      "14 1.0548827999991772\n",
      "15 1.0589098000000376\n",
      "16 1.1367444999996223\n",
      "17 1.0608317999995052\n",
      "18 1.0630043999990448\n",
      "19 1.0567005000011704\n",
      "20 1.064796299999216\n",
      "21 1.0506772000007913\n",
      "22 1.0538892000004125\n",
      "23 1.0567442000010487\n",
      "24 1.0518247999989399\n",
      "25 1.0662634000000253\n",
      "26 1.0650552000006428\n",
      "27 1.0734518999997817\n",
      "28 1.0703933000004326\n",
      "29 1.0680647999997746\n",
      "[70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167, 70.99920324894167]\n",
      "[1.7950084000003699, 1.1660730999992666, 1.057933199999752, 1.0520505999993475, 1.095794200000455, 1.0888614999985293, 1.2580463999984204, 1.0814057999996294, 1.0809062000007543, 1.0772259999994276, 1.0882581000005302, 1.078080499999487, 1.05712019999919, 1.0507240999995702, 1.0548827999991772, 1.0589098000000376, 1.1367444999996223, 1.0608317999995052, 1.0630043999990448, 1.0567005000011704, 1.064796299999216, 1.0506772000007913, 1.0538892000004125, 1.0567442000010487, 1.0518247999989399, 1.0662634000000253, 1.0650552000006428, 1.0734518999997817, 1.0703933000004326, 1.0680647999997746]\n"
     ]
    }
   ],
   "source": [
    "fastdtw_durations = []\n",
    "fastdtw_distances = []\n",
    "for iteration in range(30):\n",
    "    start_fastdtw = timer()\n",
    "    fastdtw_distance, fastdtw_path = fastdtw(series_one[\"two_sec\"], series_two[\"two_sec\"], dist=euclidean)\n",
    "    end_fastdtw = timer()\n",
    "    print(f\"{iteration} {end_fastdtw-start_fastdtw}\")\n",
    "    fastdtw_durations.append(end_fastdtw-start_fastdtw)\n",
    "    fastdtw_distances.append(fastdtw_distance)\n",
    "print(fastdtw_distances)\n",
    "print(fastdtw_durations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "dtw_comparison  = dtw_comparison.append({\"type\": \"fastdtw\",\n",
    "                                         \"distances\": fastdtw_distances,\n",
    "                                         \"durations\": fastdtw_durations},\n",
    "                                        ignore_index=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "dtw_comparison.to_csv(f\"./../dtw-comparison/dtw-comparison.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtai\n",
      "mean distance: 2.673\n",
      "std distance: 0.0\n",
      "mean duration: 27.96\n",
      "std duration: 2.325\n",
      "dtai_f\n",
      "mean distance: 2.673\n",
      "std distance: 0.0\n",
      "mean duration: 0.034\n",
      "std duration: 0.072\n",
      "fastdtw\n",
      "mean distance: 70.999\n",
      "std distance: 0.0\n",
      "mean duration: 1.103\n",
      "std duration: 0.135\n"
     ]
    }
   ],
   "source": [
    "print(\"dtai\")\n",
    "print(f\"mean distance: {round(mean(dtai_distances), 3)}\")\n",
    "print(f\"std distance: {round(np.std(dtai_distances), 3)}\")\n",
    "print(f\"mean duration: {round(mean(dtai_durations), 3)}\")\n",
    "print(f\"std duration: {round(np.std(dtai_durations), 3)}\")\n",
    "\n",
    "print(\"dtai_f\")\n",
    "print(f\"mean distance: {round(mean(dtai_f_distances), 3)}\")\n",
    "print(f\"std distance: {round(np.std(dtai_f_distances), 3)}\")\n",
    "print(f\"mean duration: {round(mean(dtai_f_durations), 3)}\")\n",
    "print(f\"std duration: {round(np.std(dtai_f_durations), 3)}\")\n",
    "\n",
    "print(\"fastdtw\")\n",
    "print(f\"mean distance: {round(mean(fastdtw_distances), 3)}\")\n",
    "print(f\"std distance: {round(np.std(fastdtw_distances), 3)}\")\n",
    "print(f\"mean duration: {round(mean(fastdtw_durations), 3)}\")\n",
    "print(f\"std duration: {round(np.std(fastdtw_durations), 3)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plots classification based on dtw measure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "series = []\n",
    "\n",
    "for f_name in function_names:\n",
    "    tmp = pd.read_csv(f\"./../{working_directory}/preprocessed-data/workers-mean-data{f_name}/translated_scaled_smoothed_data.csv\")\n",
    "    tmp = tmp['two_sec'].to_numpy()\n",
    "    series.append(tmp)\n",
    "ds = dtw.distance_matrix_fast(series)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model1 = clustering.Hierarchical(dtw.distance_matrix_fast, {})\n",
    "cluster_idx = model1.fit(series)\n",
    "model2 = clustering.HierarchicalTree(model1)\n",
    "cluster_idx = model2.fit(series)\n",
    "model3 = clustering.LinkageTree(dtw.distance_matrix_fast, {})\n",
    "cluster_idx = model3.fit(series)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1440x720 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(20, 10))  # attention: 2 columns needed!\n",
    "ax.remove()\n",
    "tree_plot = model2.plot(axes=ax,\n",
    "                        filename=\"myplot.png\",\n",
    "                        show_ts_label=True,\n",
    "                        show_tr_label=True,\n",
    "                        ts_label_margin=-40,\n",
    "                        ts_height=20,\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : /avgNetProfitGroupedBySoldDate\n",
      "1 : /avgNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "2 : /avgNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "3 : /avgWholeSaleCostGroupedBySoldDate\n",
      "4 : /countNetProfitGroupedBySoldDate\n",
      "5 : /countNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "6 : /countNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "7 : /countWholeSaleCostGroupedBySoldDate\n",
      "8 : /filterCatalogSalesWhereProfitNegative\n",
      "9 : /filterCatalogSalesWhereProfitNegativeAndYearAfter2000\n",
      "10 : /filterCatalogSalesWhereYearAfter2000\n",
      "11 : /filterStoreSalesWhereProfitNegative\n",
      "12 : /filterStoreSalesWhereProfitNegativeAndYearAfter2000\n",
      "13 : /filterStoreSalesWhereYearAfter2000\n",
      "14 : /maxNetProfitGroupedBySoldDate\n",
      "15 : /maxNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "16 : /maxNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "17 : /maxWholeSaleCostGroupedBySoldDate\n",
      "18 : /minNetProfitGroupedBySoldDate\n",
      "19 : /minNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "20 : /minNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "21 : /minWholeSaleCostGroupedBySoldDate\n",
      "22 : /summaryNetProfitGroupedBySoldDate\n",
      "23 : /summaryWholeSaleCostGroupedBySoldDate\n",
      "24 : /sumNetProfitGroupedBySoldDate\n",
      "25 : /sumNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "26 : /sumNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "27 : /sumWholeSaleCostGroupedBySoldDate\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(function_names)):\n",
    "    print(f\"{idx} : {function_names[idx]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DTW measures for for 3 example functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "9.981053067020381"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countNetProfitGroupedBySoldDate\n",
    "\n",
    "one_data_1 = pd.read_csv(f\"./../1GB-9N/preprocessed-data/workers-mean-data/countNetProfitGroupedBySoldDate/translated_scaled_smoothed_data.csv\")\n",
    "two_data_1= pd.read_csv(f\"./../2GB-9N/preprocessed-data/workers-mean-data/countNetProfitGroupedBySoldDate/translated_scaled_smoothed_data.csv\")\n",
    "\n",
    "distance_1 = dtw.distance_fast(one_data_1[\"two_sec\"].to_numpy(dtype=np.double), two_data_1[\"two_sec\"].to_numpy(dtype=np.double))\n",
    "distance_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "10.61807868185549"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maxWholeSaleCostGroupedBySoldDate\n",
    "\n",
    "one_data_2 = pd.read_csv(f\"./../1GB-9N/preprocessed-data/workers-mean-data/maxWholeSaleCostGroupedBySoldDate/translated_scaled_smoothed_data.csv\")\n",
    "two_data_2 = pd.read_csv(f\"./../2GB-9N/preprocessed-data/workers-mean-data/maxWholeSaleCostGroupedBySoldDate/translated_scaled_smoothed_data.csv\")\n",
    "\n",
    "distance_2 = dtw.distance_fast(one_data_2[\"two_sec\"].to_numpy(dtype=np.double), two_data_2[\"two_sec\"].to_numpy(dtype=np.double))\n",
    "distance_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "11.422127654265514"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summaryNetProfitGroupedBySoldDate\n",
    "\n",
    "one_data_3 = pd.read_csv(f\"./../1GB-9N/preprocessed-data/workers-mean-data/summaryNetProfitGroupedBySoldDate/translated_scaled_smoothed_data.csv\")\n",
    "two_data_3 = pd.read_csv(f\"./../2GB-9N/preprocessed-data/workers-mean-data/summaryNetProfitGroupedBySoldDate/translated_scaled_smoothed_data.csv\")\n",
    "\n",
    "distance_3 = dtw.distance_fast(one_data_3[\"two_sec\"].to_numpy(dtype=np.double),\n",
    "                               two_data_3[\"two_sec\"].to_numpy(dtype=np.double),\n",
    "                               use_pruning=True,\n",
    "                               window=10\n",
    "                               )\n",
    "distance_3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "5.327556372911023"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_4 = dtw.distance_fast(two_data_2[\"two_sec\"].to_numpy(dtype=np.double), two_data_3[\"two_sec\"].to_numpy(dtype=np.double))\n",
    "distance_4\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# figures_path_barplot = f\"./../udf-vs-cluster-config\"\n",
    "figures_path_barplot = f\"./../udf-vs-datasetsize\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Barplot with average plot length for each UDF type (cluster config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "udf_lengths = pd.read_csv(f\"{figures_path_barplot}/mean-function-types-length.csv\")\n",
    "# cluster_config = \"9N\"\n",
    "\n",
    "bar_width = 0.25\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "bars1 = udf_lengths[(udf_lengths[\"size-config\"] == f\"2GB-5N\")][\"mean_length\"]\n",
    "bars2 = udf_lengths[(udf_lengths[\"size-config\"] == f\"2GB-7N\")][\"mean_length\"]\n",
    "bars3 = udf_lengths[(udf_lengths[\"size-config\"] == f\"2GB-9N\")][\"mean_length\"]\n",
    "\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "r3 = [x + 2*bar_width for x in r1]\n",
    "\n",
    "plt.bar(r1, bars1, color='cornflowerblue', width=bar_width, edgecolor='white', label='5 węzłów')\n",
    "plt.bar(r2, bars2, color='lightcoral', width=bar_width, edgecolor='white', label='7 węzłów')\n",
    "plt.bar(r3, bars3, color='lightgreen', width=bar_width, edgecolor='white', label='9 węzłów')\n",
    "\n",
    "plt.xlabel('Typ funkcji UDF')\n",
    "plt.xticks([r + bar_width for r in range(len(bars1))], ['agregacja', 'filtracja', 'agregacja-filtracja', 'filtracja-join', 'agregacjo-filtracja-join'])\n",
    "plt.legend()\n",
    "plt.ylabel('Średnia długość charakterystyki \\n (liczba próbek)')\n",
    "plt.savefig(f\"{figures_path_barplot}/mean-length-2GB-5-7-9.png\")\n",
    "# plt.show()\n",
    "plt.close(fig)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Barplot with average plot length for each UDF type (dataset size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "udf_lengths = pd.read_csv(f\"{figures_path_barplot}/mean-function-types-length.csv\")\n",
    "cluster_config = \"9N\"\n",
    "\n",
    "bar_width = 0.25\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "bars1 = udf_lengths[(udf_lengths[\"size-config\"] == f\"1GB-{cluster_config}\")][\"mean_length\"]\n",
    "bars2 = udf_lengths[(udf_lengths[\"size-config\"] == f\"2GB-{cluster_config}\")][\"mean_length\"]\n",
    "\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "plt.bar(r1, bars1, color='cornflowerblue', width=bar_width, edgecolor='white', label='1 GB')\n",
    "plt.bar(r2, bars2, color='lightcoral', width=bar_width, edgecolor='white', label='2 GB')\n",
    "\n",
    "plt.xlabel('Typ funkcji UDF')\n",
    "plt.xticks([r + bar_width for r in range(len(bars1))], ['agregacja', 'filtracja', 'agregacja-filtracja', 'filtracja-join', 'agregacja-filtracja-join'])\n",
    "plt.legend()\n",
    "plt.ylabel('Średnia długość charakterystyki \\n (liczba próbek)')\n",
    "plt.savefig(f\"{figures_path_barplot}/mean-length-1vs2-GB-{cluster_config}.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-afd99e2c",
   "language": "python",
   "display_name": "PyCharm (opening-black-box)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}