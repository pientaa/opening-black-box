{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Time Series Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from fastdtw import fastdtw\n",
    "from statistics import mean, pstdev\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pygal\n",
    "import seaborn as sns\n",
    "import shutil"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set working directory - default is `experiments_cata`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "working_directory = \"1GB-9N\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_subdirectories(directory=\"\"):\n",
    "    subdirectories = []\n",
    "    p = Path(f\"./../{working_directory}/{directory}\")\n",
    "    for item in p.glob('*/'):\n",
    "        if item.suffix not in (['.csv', '.zip']):\n",
    "            subdirectories.append(directory + \"/\" + item.name)\n",
    "    return subdirectories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "all_directories = get_subdirectories()\n",
    "nodes_directories = [x for x in all_directories if \"node\" in x]\n",
    "\n",
    "data_directories = []\n",
    "data_directories_groups = []\n",
    "for directory in nodes_directories:\n",
    "    cur_node_subdirectories = get_subdirectories(directory)\n",
    "    data_directories.append(cur_node_subdirectories)\n",
    "\n",
    "data_directories_groups = data_directories\n",
    "data_directories = [item for sublist in data_directories for item in sublist]\n",
    "\n",
    "function_names = data_directories_groups[1]\n",
    "function_names = list(map(lambda x: x[8:], data_directories_groups[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### All functions in `experiments_data`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- avgNetProfitGroupedBySoldDate\n",
      "- avgNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- avgNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- avgWholeSaleCostGroupedBySoldDate\n",
      "- countNetProfitGroupedBySoldDate\n",
      "- countNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- countNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- countWholeSaleCostGroupedBySoldDate\n",
      "- filterCatalogSalesWhereProfitNegative\n",
      "- filterCatalogSalesWhereProfitNegativeAndYearAfter2000\n",
      "- filterCatalogSalesWhereYearAfter2000\n",
      "- filterStoreSalesWhereProfitNegative\n",
      "- filterStoreSalesWhereProfitNegativeAndYearAfter2000\n",
      "- filterStoreSalesWhereYearAfter2000\n",
      "- maxNetProfitGroupedBySoldDate\n",
      "- maxNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- maxNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- maxWholeSaleCostGroupedBySoldDate\n",
      "- minNetProfitGroupedBySoldDate\n",
      "- minNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- minNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- minWholeSaleCostGroupedBySoldDate\n",
      "- summaryNetProfitGroupedBySoldDate\n",
      "- summaryWholeSaleCostGroupedBySoldDate\n",
      "- sumNetProfitGroupedBySoldDate\n",
      "- sumNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- sumNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- sumWholeSaleCostGroupedBySoldDate\n"
     ]
    }
   ],
   "source": [
    "for function in function_names:\n",
    "    print(f\"- {function[1:]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aggregating labels by functions names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        function_name  \\\n0                       avgNetProfitGroupedBySoldDate   \n1    avgNetProfitGroupedBySoldDateWhereProfitNegative   \n2     avgNetProfitGroupedBySoldDateWhereYearAfter2000   \n3                   avgWholeSaleCostGroupedBySoldDate   \n4                           countDistinctTicketNumber   \n5                     countNetProfitGroupedBySoldDate   \n6   countNetProfitGroupedBySoldDateWhereProfitNega...   \n7   countNetProfitGroupedBySoldDateWhereYearAfter2000   \n8                 countWholeSaleCostGroupedBySoldDate   \n9               filterCatalogSalesWhereProfitNegative   \n10  filterCatalogSalesWhereProfitNegativeAndYearAf...   \n11               filterCatalogSalesWhereYearAfter2000   \n12                filterStoreSalesWhereProfitNegative   \n13  filterStoreSalesWhereProfitNegativeAndYearAfte...   \n14                 filterStoreSalesWhereYearAfter2000   \n15                      maxNetProfitGroupedBySoldDate   \n16   maxNetProfitGroupedBySoldDateWhereProfitNegative   \n17    maxNetProfitGroupedBySoldDateWhereYearAfter2000   \n18                  maxWholeSaleCostGroupedBySoldDate   \n19                      minNetProfitGroupedBySoldDate   \n20   minNetProfitGroupedBySoldDateWhereProfitNegative   \n21    minNetProfitGroupedBySoldDateWhereYearAfter2000   \n22                  minWholeSaleCostGroupedBySoldDate   \n23                      sumNetProfitGroupedBySoldDate   \n24   sumNetProfitGroupedBySoldDateWhereProfitNegative   \n25    sumNetProfitGroupedBySoldDateWhereYearAfter2000   \n26                  sumWholeSaleCostGroupedBySoldDate   \n27                  summaryNetProfitGroupedBySoldDate   \n28              summaryWholeSaleCostGroupedBySoldDate   \n\n                            label  \n0                     aggregation  \n1         aggregation, filtration  \n2   aggregation, filtration, join  \n3                     aggregation  \n4                     aggregation  \n5                     aggregation  \n6         aggregation, filtration  \n7   aggregation, filtration, join  \n8                     aggregation  \n9                      filtration  \n10               filtration, join  \n11               filtration, join  \n12                     filtration  \n13               filtration, join  \n14               filtration, join  \n15                    aggregation  \n16        aggregation, filtration  \n17  aggregation, filtration, join  \n18                    aggregation  \n19                    aggregation  \n20        aggregation, filtration  \n21  aggregation, filtration, join  \n22                    aggregation  \n23                    aggregation  \n24        aggregation, filtration  \n25  aggregation, filtration, join  \n26                    aggregation  \n27                    aggregation  \n28                    aggregation  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>function_name</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>avgNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>avgNetProfitGroupedBySoldDateWhereProfitNegative</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>avgNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>avgWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>countDistinctTicketNumber</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>countNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>countNetProfitGroupedBySoldDateWhereProfitNega...</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>countNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>countWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>filterCatalogSalesWhereProfitNegative</td>\n      <td>filtration</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>filterCatalogSalesWhereProfitNegativeAndYearAf...</td>\n      <td>filtration, join</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>filterCatalogSalesWhereYearAfter2000</td>\n      <td>filtration, join</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>filterStoreSalesWhereProfitNegative</td>\n      <td>filtration</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>filterStoreSalesWhereProfitNegativeAndYearAfte...</td>\n      <td>filtration, join</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>filterStoreSalesWhereYearAfter2000</td>\n      <td>filtration, join</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>maxNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>maxNetProfitGroupedBySoldDateWhereProfitNegative</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>maxNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>maxWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>minNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>minNetProfitGroupedBySoldDateWhereProfitNegative</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>minNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>minWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>sumNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>sumNetProfitGroupedBySoldDateWhereProfitNegative</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>sumNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>sumWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>summaryNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>summaryWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"./../notebooks/functions.csv\")\n",
    "labels = labels.groupby('function_name')['label'].apply(', '.join).reset_index()\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding symbols (ts1, ts2, ...) to function names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# iterator = 1\n",
    "# symbolic_data = pd.DataFrame(columns=[\"sym\", \"f_name\"])\n",
    "# for function in function_names:\n",
    "#     symbolic_data = symbolic_data.append({\"sym\": f\"ts{iterator}\", \"f_name\": function[1:]}, ignore_index=True)\n",
    "#     iterator += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# print(\"Legend:\")\n",
    "# labeled_data = pd.merge(symbolic_data, labels, left_on='f_name', right_on='function_name', how='left').drop('function_name', axis=1)\n",
    "# labeled_data = labeled_data\n",
    "# display(labeled_data)\n",
    "# # print(labeled_data['label'].to_string(index=False))\n",
    "# labeled_data.to_csv(f\"./../experiments_data_/preprocessed-data/corr_legend.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grouping labels by UDF type\n",
    "Plot colors:\n",
    "    * aggregation - blue,\n",
    "    * filtration - red,\n",
    "    * aggregation-filtration - green,\n",
    "    * aggregation-filtration-join - purple."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "agg_labeled = labels.loc[labels[\"label\"] == \"aggregation\"]\n",
    "agg_fil_labeled = labels.loc[labels[\"label\"] == \"aggregation, filtration\"]\n",
    "fil_labeled = labels.loc[labels[\"label\"] == \"filtration\"]\n",
    "agg_fil_join_labeled = labels.loc[labels[\"label\"] == \"aggregation, filtration, join\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Function for translating (move mean to zero) and scaling (multiply by reverse standard deviation) original data (mean from worker nodes)\n",
    "def translate_scale(dataframe):\n",
    "    dataframe[\"translated\"] = dataframe['CPU'] - dataframe['CPU'].mean()\n",
    "    dataframe[\"scaled\"] = dataframe[\"translated\"] * (1/dataframe[\"translated\"].std())\n",
    "    return dataframe\n",
    "\n",
    "def dtw_distance(x_labels, y_labels):\n",
    "    distances = []\n",
    "    for x_name in x_labels[\"function_name\"]:\n",
    "        try:\n",
    "            x_data = pd.read_csv(f\"./../{working_directory}/preprocessed-data/workers-mean-data/{x_name}/translated_scaled_smoothed_data.csv\")\n",
    "            # print(\"\\n\")\n",
    "            # print(\"------------------------\")\n",
    "            # print(f\"X: {x_name}\")\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "        for y_name in y_labels[\"function_name\"]:\n",
    "            try:\n",
    "                y_data = pd.read_csv(f\"./../{working_directory}/preprocessed-data/workers-mean-data/{y_name}/translated_scaled_smoothed_data.csv\")\n",
    "                # distance_scaled, path_scaled = fastdtw(x_data[\"scaled\"], y_data[\"scaled\"], dist=euclidean)\n",
    "                # distance_one_sec, path_one_sec = fastdtw(x_data[\"one_sec\"], y_data[\"one_sec\"], dist=euclidean)\n",
    "                # distance_two_sec, path_two_sec = fastdtw(x_data[\"two_sec\"], y_data[\"two_sec\"], dist=euclidean)\n",
    "                distance_five_sec, path_five_sec = fastdtw(x_data[\"five_sec\"], y_data[\"five_sec\"], dist=euclidean)\n",
    "                distances.append(distance_five_sec)\n",
    "                # distance_ten_sec, path_ten_sec = fastdtw(x_data[\"ten_sec\"], y_data[\"ten_sec\"], dist=euclidean)\n",
    "                # print(f\"Y: {y_name}\")\n",
    "                # print(f\"Scaled: {distance_scaled}\")\n",
    "                # print(f\"1 sec: {distance_one_sec}\")\n",
    "                # print(f\"2 sec: {distance_two_sec}\")\n",
    "                # print(f\"5 sec: {distance_five_sec}\")\n",
    "                # print(f\"10 sec: {distance_ten_sec}\")\n",
    "                # print()\n",
    "            except OSError as e:\n",
    "                print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    distances.sort()\n",
    "    distances = [i for i in distances if i != 0]\n",
    "    return round(min(distances), 2), round(max(distances), 2), round(mean(distances), 2), round(pstdev(distances), 2), distances,\n",
    "\n",
    "def group_distance_statistics(labels_X, labels_Y, name_X, name_Y, dataframe):\n",
    "    min_value, max_value, mean_value, std_value , all_distances = dtw_distance(labels_X, labels_Y)\n",
    "    dataframe = dataframe.append({\"name\": f\"X:{name_X} Y:{name_Y}\",\n",
    "                                  \"min\": min_value,\n",
    "                                  \"max\": max_value,\n",
    "                                  \"mean\": mean_value,\n",
    "                                  \"std\": std_value,\n",
    "                                  \"distances\": all_distances}, ignore_index=True)\n",
    "    print(\"\\n\")\n",
    "    print(\"----------------------\")\n",
    "    print(f\"{name_X} - {name_Y}\")\n",
    "    print(f\"MIN: {min_value}\")\n",
    "    print(f\"MAX: {max_value}\")\n",
    "    print(f\"MEAN: {mean_value}\")\n",
    "    print(f\"STD: {std_value}\")\n",
    "    return dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transforming data for Dynamic Time Warping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IN dataframe: \"CPU\"\n",
    "# OUT dataframe: \"translated\", \"scaled\", \"one_sec\", \"two_sec\", \"five_sec\", \"ten_sec\"\n",
    "\n",
    "for name in function_names:\n",
    "    file_path = f\"./../{working_directory}/preprocessed-data/workers-mean-data{name}\"\n",
    "    original_data = pd.read_csv(f\"{file_path}/mean_data.csv\")\n",
    "    original_data = original_data[[\"CPU\"]]\n",
    "    transformed_data = translate_scale(original_data)\n",
    "    transformed_data[\"one_sec\"] = transformed_data[\"scaled\"].rolling(4, min_periods=1).mean()\n",
    "    transformed_data[\"two_sec\"] = transformed_data[\"scaled\"].rolling(8, min_periods=1).mean()\n",
    "    transformed_data[\"five_sec\"] = transformed_data[\"scaled\"].rolling(20, min_periods=1).mean()\n",
    "    transformed_data[\"ten_sec\"] = transformed_data[\"scaled\"].rolling(40, min_periods=1).mean()\n",
    "    transformed_data.to_csv(f\"{file_path}/translated_scaled_smoothed_data.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "comparison_statistics = pd.DataFrame(columns=[\"name\", \"min\", \"max\", \"mean\", \"std\", \"distances\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation - aggregation\n",
      "MIN: 6.39\n",
      "MAX: 57.32\n",
      "MEAN: 23.39\n",
      "STD: 9.55\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation - filtration\n",
      "MIN: 227.98\n",
      "MAX: 468.88\n",
      "MEAN: 340.58\n",
      "STD: 72.54\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration - filtration\n",
      "MIN: 24.82\n",
      "MAX: 24.82\n",
      "MEAN: 24.82\n",
      "STD: 0.0\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation - aggregation-filtration\n",
      "MIN: 13.98\n",
      "MAX: 58.7\n",
      "MEAN: 25.11\n",
      "STD: 8.12\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration - aggregation-filtration\n",
      "MIN: 259.16\n",
      "MAX: 401.35\n",
      "MEAN: 327.09\n",
      "STD: 53.48\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation-filtration - aggregation-filtration\n",
      "MIN: 5.74\n",
      "MAX: 21.98\n",
      "MEAN: 14.21\n",
      "STD: 4.52\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation - aggregation-filtration-join\n",
      "MIN: 32.12\n",
      "MAX: 86.58\n",
      "MEAN: 54.76\n",
      "STD: 12.02\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration - aggregation-filtration-join\n",
      "MIN: 210.75\n",
      "MAX: 420.4\n",
      "MEAN: 314.03\n",
      "STD: 89.25\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation-filtration-join - aggregation-filtration-join\n",
      "MIN: 10.41\n",
      "MAX: 24.92\n",
      "MEAN: 19.36\n",
      "STD: 4.11\n"
     ]
    }
   ],
   "source": [
    "# Comparing udfs round-robin\n",
    "comparison_statistics = group_distance_statistics(agg_labeled, agg_labeled, \"aggregation\", \"aggregation\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_labeled, fil_labeled, \"aggregation\", \"filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_labeled, fil_labeled, \"filtration\", \"filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_labeled, agg_fil_labeled, \"aggregation\", \"aggregation-filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_labeled, agg_fil_labeled, \"filtration\", \"aggregation-filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_fil_labeled, agg_fil_labeled, \"aggregation-filtration\", \"aggregation-filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_labeled, agg_fil_join_labeled, \"aggregation\", \"aggregation-filtration-join\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_labeled, agg_fil_join_labeled, \"filtration\", \"aggregation-filtration-join\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_fil_join_labeled, agg_fil_join_labeled, \"aggregation-filtration-join\", \"aggregation-filtration-join\", comparison_statistics)\n",
    "\n",
    "# Saving the data  with comparison statistics to file\n",
    "comparison_statistics.to_csv(f\"./../{working_directory}/preprocessed-data/workers-mean-data/dtw_statistics.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating dtw statistics box plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "box_plot = pygal.Box(truncate_legend = 25, width = 1000, height = 1000, legend_at_bottom = True, logarithmic=True)\n",
    "box_plot.title = 'Function comparison statistics 1GB & 9 nodes'\n",
    "for iteration in range(comparison_statistics.shape[0]):\n",
    "    box_plot.add(comparison_statistics.iloc[iteration,0], comparison_statistics.iloc[iteration,5])\n",
    "box_plot.render_to_file(f\"./../{working_directory}/preprocessed-data/workers-mean-data/udf-round-robin-comparison.svg\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "del comparison_statistics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating plots for every function\n",
    "    * original - red,\n",
    "    * transformed - gree,\n",
    "    * scaled - magenta,\n",
    "    * 1 sec window - yellow,\n",
    "    * 2 sec window - cyan,\n",
    "    * 5 sec window - black,\n",
    "    * 10 sec window - blue."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "for name in function_names[0:1]:\n",
    "\n",
    "    file_path = f\"./../{working_directory}/preprocessed-data/workers-mean-data{name}\"\n",
    "    figures_path = f\"{file_path}/dtw\"\n",
    "    try:\n",
    "        os.mkdir(figures_path)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    read_data = pd.read_csv(f\"{file_path}/translated_scaled_smoothed_data.csv\")\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"CPU\"], 'r.-', label=\"original\")\n",
    "    plt.plot(read_data[\"translated\"], 'g.-', label=\"translated\")\n",
    "    plt.plot(read_data[\"scaled\"], 'm.-', label=\"scaled\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: original, translated, scaled\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/org_trans_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"one_sec\"], 'y-', label=\"1s window\")\n",
    "    plt.plot(read_data[\"two_sec\"], 'c-', label=\"2s window\")\n",
    "    plt.plot(read_data[\"five_sec\"], 'k-', label=\"5s window\")\n",
    "    plt.plot(read_data[\"ten_sec\"], 'b-', label=\"10s window\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 1s, 2s, 5s, 10s)\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/scaled_dtw_smoothed.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"one_sec\"], 'y-', label=\"1s window\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 1s)\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/1_sec_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"two_sec\"], 'c-', label=\"2s window\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 2s)\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/2_sec_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], '-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"five_sec\"], '-', label=\"5s window\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 5s)\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/5_sec_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], '-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"ten_sec\"], '-', label=\"10s window\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 10s)\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/10_sec_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # fig = plt.figure(figsize=(20,10))\n",
    "    # fig.patch.set_facecolor('white')\n",
    "    # plt.plot(read_data[\"five_sec\"], '-', label=\"five_sec\")\n",
    "    # plt.title(f\"{name[1:]} | ten_sec |\")\n",
    "    # plt.legend()\n",
    "    # plt.savefig(f\"{figures_path}/final_5_sec.png\")\n",
    "    # plt.close(fig)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "for function_name in function_names:\n",
    "    try:\n",
    "        shutil.rmtree(f\"./../{working_directory}/preprocessed-data/workers-mean-data{function_name}/dtw\")\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def plot_udf_type(udf_type, udf_type_name):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    for name in udf_type[\"function_name\"]:\n",
    "        try:\n",
    "            file_path = f\"./../{working_directory}/preprocessed-data/workers-mean-data/{name}\"\n",
    "            read_data = pd.read_csv(f\"{file_path}/translated_scaled_smoothed_data.csv\")\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "        plt.plot(read_data[\"five_sec\"], '-', label=name)\n",
    "    plt.title(f\" 5s window | {udf_type_name}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./../{working_directory}/preprocessed-data/workers-mean-data/{udf_type_name}.png\")\n",
    "    plt.close(fig)\n",
    "    # plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countDistinctTicketNumber/translated_scaled_smoothed_data.csv - No such file or directory.\n"
     ]
    }
   ],
   "source": [
    "# agg_labeled agg_fil_labeled fil_labeled agg_fil_join_labeled\n",
    "plot_udf_type(agg_labeled, \"aggregation\")\n",
    "plot_udf_type(fil_labeled, \"filtration\")\n",
    "plot_udf_type(agg_fil_labeled, \"aggregation-filtration\")\n",
    "plot_udf_type(agg_fil_join_labeled, \"aggregation-filtration-join\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean up"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Deleting created files\n",
    "\n",
    "for function_name in function_names:\n",
    "    try:\n",
    "        os.remove(f'./../{working_directory}/preprocessed-data/workers-mean-data{function_name}/translated_scaled_smoothed_data.csv')\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-afd99e2c",
   "language": "python",
   "display_name": "PyCharm (opening-black-box)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}