{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Time Series Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from fastdtw import fastdtw\n",
    "from statistics import mean, pstdev\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pygal\n",
    "import seaborn as sns\n",
    "import shutil"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set working directory - default is `experiments_cata`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "working_directory = \"1GB-7N\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_subdirectories(directory=\"\"):\n",
    "    subdirectories = []\n",
    "    p = Path(f\"./../{working_directory}/{directory}\")\n",
    "    for item in p.glob('*/'):\n",
    "        if item.suffix not in (['.csv', '.zip']):\n",
    "            subdirectories.append(directory + \"/\" + item.name)\n",
    "    return subdirectories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "all_directories = get_subdirectories()\n",
    "nodes_directories = [x for x in all_directories if \"node\" in x]\n",
    "\n",
    "data_directories = []\n",
    "data_directories_groups = []\n",
    "for directory in nodes_directories:\n",
    "    cur_node_subdirectories = get_subdirectories(directory)\n",
    "    data_directories.append(cur_node_subdirectories)\n",
    "\n",
    "data_directories_groups = data_directories\n",
    "data_directories = [item for sublist in data_directories for item in sublist]\n",
    "\n",
    "function_names = data_directories_groups[1]\n",
    "function_names = list(map(lambda x: x[8:], data_directories_groups[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### All functions in `experiments_data`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- avgNetProfitGroupedBySoldDate\n",
      "- avgNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- avgNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- avgWholeSaleCostGroupedBySoldDate\n",
      "- countDistinctTicketNumber\n",
      "- countNetProfitGroupedBySoldDate\n",
      "- countNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- countNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- countWholeSaleCostGroupedBySoldDate\n",
      "- filterCatalogSalesWhereProfitNegative\n",
      "- filterCatalogSalesWhereProfitNegativeAndYearAfter2000\n",
      "- filterCatalogSalesWhereYearAfter2000\n",
      "- filterStoreSalesWhereProfitNegative\n",
      "- filterStoreSalesWhereProfitNegativeAndYearAfter2000\n",
      "- filterStoreSalesWhereYearAfter2000\n",
      "- maxNetProfitGroupedBySoldDate\n",
      "- maxNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- maxNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- maxWholeSaleCostGroupedBySoldDate\n",
      "- minNetProfitGroupedBySoldDate\n",
      "- minNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- minNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- minWholeSaleCostGroupedBySoldDate\n",
      "- summaryNetProfitGroupedBySoldDate\n",
      "- summaryWholeSaleCostGroupedBySoldDate\n",
      "- sumNetProfitGroupedBySoldDate\n",
      "- sumNetProfitGroupedBySoldDateWhereProfitNegative\n",
      "- sumNetProfitGroupedBySoldDateWhereYearAfter2000\n",
      "- sumWholeSaleCostGroupedBySoldDate\n"
     ]
    }
   ],
   "source": [
    "for function in function_names:\n",
    "    print(f\"- {function[1:]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aggregating labels by functions names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        function_name  \\\n0                       avgNetProfitGroupedBySoldDate   \n1    avgNetProfitGroupedBySoldDateWhereProfitNegative   \n2     avgNetProfitGroupedBySoldDateWhereYearAfter2000   \n3                   avgWholeSaleCostGroupedBySoldDate   \n4                     countNetProfitGroupedBySoldDate   \n5   countNetProfitGroupedBySoldDateWhereProfitNega...   \n6   countNetProfitGroupedBySoldDateWhereYearAfter2000   \n7                 countWholeSaleCostGroupedBySoldDate   \n8               filterCatalogSalesWhereProfitNegative   \n9   filterCatalogSalesWhereProfitNegativeAndYearAf...   \n10               filterCatalogSalesWhereYearAfter2000   \n11                filterStoreSalesWhereProfitNegative   \n12  filterStoreSalesWhereProfitNegativeAndYearAfte...   \n13                 filterStoreSalesWhereYearAfter2000   \n14                      maxNetProfitGroupedBySoldDate   \n15   maxNetProfitGroupedBySoldDateWhereProfitNegative   \n16    maxNetProfitGroupedBySoldDateWhereYearAfter2000   \n17                  maxWholeSaleCostGroupedBySoldDate   \n18                      minNetProfitGroupedBySoldDate   \n19   minNetProfitGroupedBySoldDateWhereProfitNegative   \n20    minNetProfitGroupedBySoldDateWhereYearAfter2000   \n21                  minWholeSaleCostGroupedBySoldDate   \n22                      sumNetProfitGroupedBySoldDate   \n23   sumNetProfitGroupedBySoldDateWhereProfitNegative   \n24    sumNetProfitGroupedBySoldDateWhereYearAfter2000   \n25                  sumWholeSaleCostGroupedBySoldDate   \n26                  summaryNetProfitGroupedBySoldDate   \n27              summaryWholeSaleCostGroupedBySoldDate   \n\n                            label  \n0                     aggregation  \n1         aggregation, filtration  \n2   aggregation, filtration, join  \n3                     aggregation  \n4                     aggregation  \n5         aggregation, filtration  \n6   aggregation, filtration, join  \n7                     aggregation  \n8                      filtration  \n9                filtration, join  \n10               filtration, join  \n11                     filtration  \n12               filtration, join  \n13               filtration, join  \n14                    aggregation  \n15        aggregation, filtration  \n16  aggregation, filtration, join  \n17                    aggregation  \n18                    aggregation  \n19        aggregation, filtration  \n20  aggregation, filtration, join  \n21                    aggregation  \n22                    aggregation  \n23        aggregation, filtration  \n24  aggregation, filtration, join  \n25                    aggregation  \n26                    aggregation  \n27                    aggregation  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>function_name</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>avgNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>avgNetProfitGroupedBySoldDateWhereProfitNegative</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>avgNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>avgWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>countNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>countNetProfitGroupedBySoldDateWhereProfitNega...</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>countNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>countWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>filterCatalogSalesWhereProfitNegative</td>\n      <td>filtration</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>filterCatalogSalesWhereProfitNegativeAndYearAf...</td>\n      <td>filtration, join</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>filterCatalogSalesWhereYearAfter2000</td>\n      <td>filtration, join</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>filterStoreSalesWhereProfitNegative</td>\n      <td>filtration</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>filterStoreSalesWhereProfitNegativeAndYearAfte...</td>\n      <td>filtration, join</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>filterStoreSalesWhereYearAfter2000</td>\n      <td>filtration, join</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>maxNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>maxNetProfitGroupedBySoldDateWhereProfitNegative</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>maxNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>maxWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>minNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>minNetProfitGroupedBySoldDateWhereProfitNegative</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>minNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>minWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>sumNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>sumNetProfitGroupedBySoldDateWhereProfitNegative</td>\n      <td>aggregation, filtration</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>sumNetProfitGroupedBySoldDateWhereYearAfter2000</td>\n      <td>aggregation, filtration, join</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>sumWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>summaryNetProfitGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>summaryWholeSaleCostGroupedBySoldDate</td>\n      <td>aggregation</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"./../notebooks/functions.csv\")\n",
    "labels = labels.groupby('function_name')['label'].apply(', '.join).reset_index()\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding symbols (ts1, ts2, ...) to function names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# iterator = 1\n",
    "# symbolic_data = pd.DataFrame(columns=[\"sym\", \"f_name\"])\n",
    "# for function in function_names:\n",
    "#     symbolic_data = symbolic_data.append({\"sym\": f\"ts{iterator}\", \"f_name\": function[1:]}, ignore_index=True)\n",
    "#     iterator += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# print(\"Legend:\")\n",
    "# labeled_data = pd.merge(symbolic_data, labels, left_on='f_name', right_on='function_name', how='left').drop('function_name', axis=1)\n",
    "# labeled_data = labeled_data\n",
    "# display(labeled_data)\n",
    "# # print(labeled_data['label'].to_string(index=False))\n",
    "# labeled_data.to_csv(f\"./../experiments_data_/preprocessed-data/corr_legend.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grouping labels by UDF type\n",
    "Plot colors:\n",
    "    * aggregation - blue,\n",
    "    * filtration - red,\n",
    "    * aggregation-filtration - green,\n",
    "    * aggregation-filtration-join - purple."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "agg_labeled = labels.loc[labels[\"label\"] == \"aggregation\"]\n",
    "agg_fil_labeled = labels.loc[labels[\"label\"] == \"aggregation, filtration\"]\n",
    "fil_labeled = labels.loc[labels[\"label\"] == \"filtration\"]\n",
    "agg_fil_join_labeled = labels.loc[labels[\"label\"] == \"aggregation, filtration, join\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Function for translating (move mean to zero) and scaling (multiply by reverse standard deviation) original data (mean from worker nodes)\n",
    "def translate_scale(dataframe):\n",
    "    dataframe[\"translated\"] = dataframe['CPU'] - dataframe['CPU'].mean()\n",
    "    dataframe[\"scaled\"] = dataframe[\"translated\"] * (1/dataframe[\"translated\"].std())\n",
    "    return dataframe\n",
    "\n",
    "def dtw_distance(x_labels, y_labels):\n",
    "    distances = []\n",
    "    for x_name in x_labels[\"function_name\"]:\n",
    "        try:\n",
    "            x_data = pd.read_csv(f\"./../{working_directory}/preprocessed-data/workers-mean-data/{x_name}/translated_scaled_smoothed_data.csv\")\n",
    "            # print(\"\\n\")\n",
    "            # print(\"------------------------\")\n",
    "            # print(f\"X: {x_name}\")\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "        for y_name in y_labels[\"function_name\"]:\n",
    "            try:\n",
    "                y_data = pd.read_csv(f\"./../{working_directory}/preprocessed-data/workers-mean-data/{y_name}/translated_scaled_smoothed_data.csv\")\n",
    "                # distance_scaled, path_scaled = fastdtw(x_data[\"scaled\"], y_data[\"scaled\"], dist=euclidean)\n",
    "                # distance_one_sec, path_one_sec = fastdtw(x_data[\"one_sec\"], y_data[\"one_sec\"], dist=euclidean)\n",
    "                # distance_two_sec, path_two_sec = fastdtw(x_data[\"two_sec\"], y_data[\"two_sec\"], dist=euclidean)\n",
    "                distance_five_sec, path_five_sec = fastdtw(x_data[\"five_sec\"], y_data[\"five_sec\"], dist=euclidean)\n",
    "                distances.append(distance_five_sec)\n",
    "                # distance_ten_sec, path_ten_sec = fastdtw(x_data[\"ten_sec\"], y_data[\"ten_sec\"], dist=euclidean)\n",
    "                # print(f\"Y: {y_name}\")\n",
    "                # print(f\"Scaled: {distance_scaled}\")\n",
    "                # print(f\"1 sec: {distance_one_sec}\")\n",
    "                # print(f\"2 sec: {distance_two_sec}\")\n",
    "                # print(f\"5 sec: {distance_five_sec}\")\n",
    "                # print(f\"10 sec: {distance_ten_sec}\")\n",
    "                # print()\n",
    "            except OSError as e:\n",
    "                print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    distances.sort()\n",
    "    distances = [i for i in distances if i != 0]\n",
    "    return round(min(distances), 2), round(max(distances), 2), round(mean(distances), 2), round(pstdev(distances), 2), distances,\n",
    "\n",
    "def group_distance_statistics(labels_X, labels_Y, name_X, name_Y, dataframe):\n",
    "    min_value, max_value, mean_value, std_value , all_distances = dtw_distance(labels_X, labels_Y)\n",
    "    dataframe = dataframe.append({\"name\": f\"X:{name_X} Y:{name_Y}\",\n",
    "                                  \"min\": min_value,\n",
    "                                  \"max\": max_value,\n",
    "                                  \"mean\": mean_value,\n",
    "                                  \"std\": std_value,\n",
    "                                  \"distances\": all_distances}, ignore_index=True)\n",
    "    print(\"\\n\")\n",
    "    print(\"----------------------\")\n",
    "    print(f\"{name_X} - {name_Y}\")\n",
    "    print(f\"MIN: {min_value}\")\n",
    "    print(f\"MAX: {max_value}\")\n",
    "    print(f\"MEAN: {mean_value}\")\n",
    "    print(f\"STD: {std_value}\")\n",
    "    return dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transforming data for Dynamic Time Warping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './../2GB-7N/preprocessed-data/workers-mean-data/avgNetProfitGroupedBySoldDate/mean_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-621c3f0d80c8>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfunction_names\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mfile_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mf\"./../{working_directory}/preprocessed-data/workers-mean-data{name}\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0moriginal_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"{file_path}/mean_data.csv\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m     \u001B[0moriginal_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moriginal_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"CPU\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[0mtransformed_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtranslate_scale\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moriginal_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\data-processing\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    608\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    609\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 610\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    611\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    612\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\data-processing\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    460\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    461\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 462\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    463\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    464\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\data-processing\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    817\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    818\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 819\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    820\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    821\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\data-processing\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1048\u001B[0m             )\n\u001B[0;32m   1049\u001B[0m         \u001B[1;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1050\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1051\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1052\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\data-processing\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m   1865\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1866\u001B[0m         \u001B[1;31m# open handles\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1867\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1868\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1869\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m\"storage_options\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"encoding\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"memory_map\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"compression\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\data-processing\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_open_handles\u001B[1;34m(self, src, kwds)\u001B[0m\n\u001B[0;32m   1360\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHanldes\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1361\u001B[0m         \"\"\"\n\u001B[1;32m-> 1362\u001B[1;33m         self.handles = get_handle(\n\u001B[0m\u001B[0;32m   1363\u001B[0m             \u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1364\u001B[0m             \u001B[1;34m\"r\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\envs\\data-processing\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    640\u001B[0m                 \u001B[0merrors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"replace\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    641\u001B[0m             \u001B[1;31m# Encoding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 642\u001B[1;33m             handle = open(\n\u001B[0m\u001B[0;32m    643\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    644\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './../2GB-7N/preprocessed-data/workers-mean-data/avgNetProfitGroupedBySoldDate/mean_data.csv'"
     ]
    }
   ],
   "source": [
    "# IN dataframe: \"CPU\"\n",
    "# OUT dataframe: \"translated\", \"scaled\", \"one_sec\", \"two_sec\", \"five_sec\", \"ten_sec\"\n",
    "\n",
    "for name in function_names:\n",
    "    file_path = f\"./../{working_directory}/preprocessed-data/workers-mean-data{name}\"\n",
    "    original_data = pd.read_csv(f\"{file_path}/mean_data.csv\")\n",
    "    original_data = original_data[[\"CPU\"]]\n",
    "    transformed_data = translate_scale(original_data)\n",
    "    transformed_data[\"one_sec\"] = transformed_data[\"scaled\"].rolling(4, min_periods=1).mean()\n",
    "    transformed_data[\"two_sec\"] = transformed_data[\"scaled\"].rolling(8, min_periods=1).mean()\n",
    "    transformed_data[\"five_sec\"] = transformed_data[\"scaled\"].rolling(20, min_periods=10, center=True).mean()\n",
    "    transformed_data[\"ten_sec\"] = transformed_data[\"scaled\"].rolling(40, min_periods=1, center=True).mean()\n",
    "    transformed_data.to_csv(f\"{file_path}/translated_scaled_smoothed_data.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-38-994632aa63e3>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mdel\u001B[0m \u001B[0mtransformed_data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'transformed_data' is not defined"
     ]
    }
   ],
   "source": [
    "del transformed_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "comparison_statistics = pd.DataFrame(columns=[\"name\", \"min\", \"max\", \"mean\", \"std\", \"distances\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------\n",
      "aggregation - aggregation\n",
      "MIN: 6.34\n",
      "MAX: 65.5\n",
      "MEAN: 23.58\n",
      "STD: 10.11\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation - filtration\n",
      "MIN: 198.22\n",
      "MAX: 245.28\n",
      "MEAN: 223.43\n",
      "STD: 14.74\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration - filtration\n",
      "MIN: 25.91\n",
      "MAX: 25.91\n",
      "MEAN: 25.91\n",
      "STD: 0.0\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation - aggregation-filtration\n",
      "MIN: 14.49\n",
      "MAX: 61.09\n",
      "MEAN: 25.63\n",
      "STD: 8.8\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration - aggregation-filtration\n",
      "MIN: 194.16\n",
      "MAX: 237.02\n",
      "MEAN: 214.74\n",
      "STD: 15.37\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation-filtration - aggregation-filtration\n",
      "MIN: 6.79\n",
      "MAX: 21.99\n",
      "MEAN: 14.54\n",
      "STD: 4.38\n",
      "\n",
      "\n",
      "----------------------\n",
      "aggregation - aggregation-filtration-join\n",
      "MIN: 34.29\n",
      "MAX: 78.48\n",
      "MEAN: 55.93\n",
      "STD: 10.54\n",
      "\n",
      "\n",
      "----------------------\n",
      "filtration - aggregation-filtration-join\n",
      "MIN: 194.15\n",
      "MAX: 248.31\n",
      "MEAN: 222.63\n",
      "STD: 16.62\n"
     ]
    }
   ],
   "source": [
    "# Comparing udfs round-robin\n",
    "comparison_statistics = group_distance_statistics(agg_labeled, agg_labeled, \"aggregation\", \"aggregation\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_labeled, fil_labeled, \"aggregation\", \"filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_labeled, fil_labeled, \"filtration\", \"filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_labeled, agg_fil_labeled, \"aggregation\", \"aggregation-filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_labeled, agg_fil_labeled, \"filtration\", \"aggregation-filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_fil_labeled, agg_fil_labeled, \"aggregation-filtration\", \"aggregation-filtration\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(agg_labeled, agg_fil_join_labeled, \"aggregation\", \"aggregation-filtration-join\", comparison_statistics)\n",
    "comparison_statistics = group_distance_statistics(fil_labeled, agg_fil_join_labeled, \"filtration\", \"aggregation-filtration-join\", comparison_statistics)\n",
    "# comparison_statistics = group_distance_statistics(agg_fil_join_labeled, agg_fil_join_labeled, \"aggregation-filtration-join\", \"aggregation-filtration-join\", comparison_statistics)\n",
    "\n",
    "# Saving the data  with comparison statistics to file\n",
    "comparison_statistics.to_csv(f\"./../{working_directory}/preprocessed-data/workers-mean-data/dtw_statistics.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating dtw statistics box plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "box_plot = pygal.Box(truncate_legend = 25, width = 1000, height = 1000, legend_at_bottom = True, logarithmic=True)\n",
    "box_plot.title = f\"Function comparison statistics {working_directory}odes\"\n",
    "for iteration in range(comparison_statistics.shape[0]):\n",
    "    box_plot.add(comparison_statistics.iloc[iteration,0], comparison_statistics.iloc[iteration,5])\n",
    "box_plot.render_to_file(f\"./../{working_directory}/preprocessed-data/workers-mean-data/udf-round-robin-comparison.svg\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "del comparison_statistics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating plots for every function\n",
    "    * original - red,\n",
    "    * transformed - green,\n",
    "    * scaled - magenta,\n",
    "    * 1 sec window - yellow,\n",
    "    * 2 sec window - cyan,\n",
    "    * 5 sec window - black,\n",
    "    * 10 sec window - blue."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "for name in function_names:\n",
    "\n",
    "    file_path = f\"./../{working_directory}/preprocessed-data/workers-mean-data{name}\"\n",
    "    figures_path = f\"{file_path}/dtw\"\n",
    "    try:\n",
    "        os.mkdir(figures_path)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    read_data = pd.read_csv(f\"{file_path}/translated_scaled_smoothed_data.csv\")\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"CPU\"], 'r.-', label=\"original\")\n",
    "    plt.plot(read_data[\"translated\"], 'g.-', label=\"translated\")\n",
    "    plt.plot(read_data[\"scaled\"], 'm.-', label=\"scaled\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: original, translated, scaled\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/org_trans_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"one_sec\"], 'y-', label=\"1s window\")\n",
    "    plt.plot(read_data[\"two_sec\"], 'c-', label=\"2s window\")\n",
    "    plt.plot(read_data[\"five_sec\"], 'k-', label=\"5s window\")\n",
    "    plt.plot(read_data[\"ten_sec\"], 'b-', label=\"10s window\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 1s, 2s, 5s, 10s)\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/scaled_dtw_smoothed.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"one_sec\"], 'y-', label=\"1s window\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 1s)\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/1_sec_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"two_sec\"], 'c-', label=\"2s window\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 2s)\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/2_sec_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"five_sec\"], 'k-', label=\"5s window\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 5s)\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/5_sec_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(read_data[\"scaled\"], 'm-', label=\"scaled\")\n",
    "    plt.plot(read_data[\"ten_sec\"], 'b-', label=\"10s window\")\n",
    "    plt.title(f\"Function: {name[1:]} | Plots: scaled, smoothed (window: 10s)\", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/10_sec_scaled.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # fig = plt.figure(figsize=(20,10))\n",
    "    # fig.patch.set_facecolor('white')\n",
    "    # plt.plot(read_data[\"five_sec\"], '-', label=\"five_sec\")\n",
    "    # plt.title(f\"{name[1:]} | ten_sec |\")\n",
    "    # plt.legend()\n",
    "    # plt.savefig(f\"{figures_path}/final_5_sec.png\")\n",
    "    # plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def plot_udf_type(udf_type, udf_type_name):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    for name in udf_type[\"function_name\"]:\n",
    "        try:\n",
    "            file_path = f\"./../{working_directory}/preprocessed-data/workers-mean-data/{name}\"\n",
    "            read_data = pd.read_csv(f\"{file_path}/translated_scaled_smoothed_data.csv\")\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "        plt.plot(read_data[\"five_sec\"], '-', label=name)\n",
    "    plt.title(f\" 5s window | {udf_type_name}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"./../{working_directory}/preprocessed-data/workers-mean-data/{udf_type_name}.png\")\n",
    "    plt.close(fig)\n",
    "    # plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# agg_labeled agg_fil_labeled fil_labeled agg_fil_join_labeled\n",
    "plot_udf_type(agg_labeled, \"aggregation\")\n",
    "plot_udf_type(fil_labeled, \"filtration\")\n",
    "plot_udf_type(agg_fil_labeled, \"aggregation-filtration\")\n",
    "plot_udf_type(agg_fil_join_labeled, \"aggregation-filtration-join\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparing data from 1GB and 2GB datasete sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "figures_path = f\"./../{working_directory}/preprocessed-data/workers-mean-data/1GBvs2GB\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(figures_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "for name in function_names:\n",
    "    one_gb_data = pd.read_csv(f\"./../1GB-9N/preprocessed-data/workers-mean-data{name}/translated_scaled_smoothed_data.csv\")\n",
    "    two_gb_data = pd.read_csv(f\"./../2GB-9N/preprocessed-data/workers-mean-data{name}/translated_scaled_smoothed_data.csv\")\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(one_gb_data[\"five_sec\"], 'k-', label=f\"1 GB\")\n",
    "    plt.plot(two_gb_data[\"five_sec\"], '-', label=f\"2 GB\", color=\"olive\")\n",
    "    plt.title(f\"Function: {name[1:]} | 1GB | 2GB \", fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{figures_path}/{name[1:]}_1GBvs2GB.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean up"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Deleting created files\n",
    "\n",
    "for function_name in function_names:\n",
    "    try:\n",
    "        os.remove(f\"./../{working_directory}/preprocessed-data/workers-mean-data{function_name}/translated_scaled_smoothed_data.csv\")\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/avgNetProfitGroupedBySoldDateWhereProfitNegative/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/avgNetProfitGroupedBySoldDateWhereYearAfter2000/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/avgWholeSaleCostGroupedBySoldDate/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countNetProfitGroupedBySoldDate/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countNetProfitGroupedBySoldDateWhereProfitNegative/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countNetProfitGroupedBySoldDateWhereYearAfter2000/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/countWholeSaleCostGroupedBySoldDate/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/filterCatalogSalesWhereProfitNegative/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/filterCatalogSalesWhereProfitNegativeAndYearAfter2000/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/filterCatalogSalesWhereYearAfter2000/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/filterStoreSalesWhereProfitNegative/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/filterStoreSalesWhereProfitNegativeAndYearAfter2000/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/filterStoreSalesWhereYearAfter2000/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/maxNetProfitGroupedBySoldDate/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/maxNetProfitGroupedBySoldDateWhereProfitNegative/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/maxNetProfitGroupedBySoldDateWhereYearAfter2000/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/maxWholeSaleCostGroupedBySoldDate/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/minNetProfitGroupedBySoldDate/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/minNetProfitGroupedBySoldDateWhereProfitNegative/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/minNetProfitGroupedBySoldDateWhereYearAfter2000/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/minWholeSaleCostGroupedBySoldDate/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/summaryNetProfitGroupedBySoldDate/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/summaryWholeSaleCostGroupedBySoldDate/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/sumNetProfitGroupedBySoldDate/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/sumNetProfitGroupedBySoldDateWhereProfitNegative/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/sumNetProfitGroupedBySoldDateWhereYearAfter2000/dtw - System nie może odnaleźć określonej ścieżki.\n",
      "Error: ./../1GB-9N/preprocessed-data/workers-mean-data/sumWholeSaleCostGroupedBySoldDate/dtw - System nie może odnaleźć określonej ścieżki.\n"
     ]
    }
   ],
   "source": [
    "for function_name in function_names:\n",
    "    try:\n",
    "        shutil.rmtree(f\"./../{working_directory}/preprocessed-data/workers-mean-data{function_name}/dtw\")\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-afd99e2c",
   "language": "python",
   "display_name": "PyCharm (opening-black-box)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}