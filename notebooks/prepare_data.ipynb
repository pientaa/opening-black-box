{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "prepare_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "FQnQig-Tqwrs"
   },
   "source": [
    "import pandas as pd   \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tsmoothie.smoother import *\n",
    "from statistics import mean"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook for preprocessing data from experiments\n",
    "\n",
    "Defined functions:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_subdirectories(directory=\"\"):\n",
    "    subdirectories = []\n",
    "    p = Path(\"./../experiments_data/\" + directory)\n",
    "    for item in p.glob('*/'):\n",
    "        if item.suffix not in (['.csv', '.zip']):\n",
    "            subdirectories.append(directory + \"/\" + item.name)\n",
    "    return subdirectories\n",
    "\n",
    "def get_timestamp_info(data):\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    start_time = data['timestamp'].min()\n",
    "    stop_time = data['timestamp'].max()\n",
    "\n",
    "    data['timestamp'] = pd.to_numeric(data['timestamp'])\n",
    "    mean_interval = mean(data.diff(axis=0)['timestamp'][1:].tolist())\n",
    "    mean_interval = round(mean_interval / 1000000000, 3)\n",
    "\n",
    "    return (stop_time - start_time).total_seconds(), mean_interval\n",
    "\n",
    "def preprocess_file(file_path, save_path, filename, iteration):\n",
    "    read_data = pd.read_csv(file_path + \"/\" + filename)\n",
    "    agg_read_data = read_data.\\\n",
    "        groupby('timestamp', as_index=False).\\\n",
    "        agg({\"CPU\": \"sum\", \"RAM\": \"sum\"})\n",
    "\n",
    "    generate_plot(agg_read_data.index, agg_read_data.CPU, \"{0}/CPU/{1}_CPU.png\".format(save_path, iteration))\n",
    "    generate_plot(agg_read_data.index, agg_read_data.RAM, \"{0}/RAM/{1}_RAM.png\".format(save_path, iteration))\n",
    "\n",
    "    return agg_read_data, get_timestamp_info(agg_read_data)\n",
    "\n",
    "def generate_plot(data_x=\"\", data_y=\"\", plot_path=\"\", title=\"\"):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(data_x, data_y, 'r.-')\n",
    "    plt.title(title)\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "def create_directory(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "def smooth_data(mean_data, path, function_name):\n",
    "        smoother = ConvolutionSmoother(window_len=7, window_type=\"ones\")\n",
    "        cpu_data = mean_data['CPU']\n",
    "        ram_data = mean_data['RAM']\n",
    "\n",
    "        smoother.smooth(cpu_data)\n",
    "        smooth_cpu_data = smoother.smooth_data[0]\n",
    "        smoother.smooth(ram_data)\n",
    "        smooth_ram_data = smoother.smooth_data[0]\n",
    "\n",
    "        generate_plot(data_x=mean_data.index,\n",
    "                      data_y=smooth_cpu_data,\n",
    "                      plot_path=\"{0}/smooth_mean_CPU.png\".format(path),\n",
    "                      title=\"Smoothed {0}\".format(function_name))\n",
    "        generate_plot(data_x=mean_data.index,\n",
    "                      data_y=smooth_ram_data,\n",
    "                      plot_path=\"{0}/smooth_mean_RAM.png\".format(path),\n",
    "                      title=\"Smoothed {0}\".format(function_name))\n",
    "\n",
    "        smooth_data = pd.DataFrame(list(zip(smooth_cpu_data, smooth_ram_data)), columns=['CPU', 'RAM'])\n",
    "        smooth_data.to_csv(\"{0}/smooth_mean_data.csv\".format(path), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check for subdirectories. Number of subdirectories should much the number of nodes used for experiments."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['/node_17',\n '/node_20',\n '/node_13',\n '/node_18',\n '/node_19',\n '/node_16',\n '/node_11',\n '/node_15',\n '/node_14',\n '/node_12']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_directories = get_subdirectories()\n",
    "nodes_directories = [x for x in all_directories if \"node\" in x]\n",
    "nodes_directories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[['/node_17/minNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_17/countNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_17/sumNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_17/maxNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_17/avgNetProfitGroupedBySoldDateWhereProfitNegative'],\n ['/node_20/minNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_20/countNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_20/sumNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_20/maxNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_20/avgNetProfitGroupedBySoldDateWhereProfitNegative'],\n ['/node_13/minNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_13/countNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_13/sumNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_13/maxNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_13/avgNetProfitGroupedBySoldDateWhereProfitNegative'],\n ['/node_18/minNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_18/countNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_18/sumNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_18/maxNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_18/avgNetProfitGroupedBySoldDateWhereProfitNegative'],\n ['/node_19/minNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_19/countNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_19/sumNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_19/maxNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_19/avgNetProfitGroupedBySoldDateWhereProfitNegative'],\n ['/node_16/minNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_16/countNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_16/sumNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_16/maxNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_16/avgNetProfitGroupedBySoldDateWhereProfitNegative'],\n ['/node_11/minNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_11/countNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_11/sumNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_11/maxNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_11/avgNetProfitGroupedBySoldDateWhereProfitNegative'],\n ['/node_15/minNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_15/countNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_15/sumNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_15/maxNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_15/avgNetProfitGroupedBySoldDateWhereProfitNegative'],\n ['/node_14/minNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_14/countNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_14/sumNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_14/maxNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_14/avgNetProfitGroupedBySoldDateWhereProfitNegative'],\n ['/node_12/minNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_12/countNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_12/sumNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_12/maxNetProfitGroupedBySoldDateWhereProfitNegative',\n  '/node_12/avgNetProfitGroupedBySoldDateWhereProfitNegative']]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_directories = []\n",
    "data_directories_groups = []\n",
    "for directory in nodes_directories:\n",
    "    cur_node_subdirectories = get_subdirectories(directory)\n",
    "    data_directories.append(cur_node_subdirectories)\n",
    "\n",
    "data_directories_groups = data_directories\n",
    "data_directories = [item for sublist in data_directories for item in sublist]\n",
    "data_directories_groups"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check for function names in each node directory.\n",
    "These names should match the names in `experiments-plan.csv`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['/minNetProfitGroupedBySoldDateWhereProfitNegative',\n '/countNetProfitGroupedBySoldDateWhereProfitNegative',\n '/sumNetProfitGroupedBySoldDateWhereProfitNegative',\n '/maxNetProfitGroupedBySoldDateWhereProfitNegative',\n '/avgNetProfitGroupedBySoldDateWhereProfitNegative']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_names = data_directories_groups[1]\n",
    "function_names = list(map(lambda x: x[8:], data_directories_groups[1]))\n",
    "function_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aggregating data from all nodes (11-19)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "create_directory(\"./../experiments_data/preprocessed-data\")\n",
    "\n",
    "mean_dir = \"./../experiments_data/preprocessed-data/workers-mean-data\"\n",
    "create_directory(mean_dir)\n",
    "workers_summary = pd.DataFrame(columns=[\"function_name\",\n",
    "                                             \"mean_duration\",\n",
    "                                             \"mean_interval\",\n",
    "                                             \"nodes_durations\",\n",
    "                                             \"nodes_intervals\"])\n",
    "\n",
    "for function_name in function_names:\n",
    "    base_data = pd.DataFrame()\n",
    "    experiment_duration = []\n",
    "    experiment_interval = []\n",
    "    nodes_durations = []\n",
    "    nodes_intervals = []\n",
    "\n",
    "    experiment_mean_dir = \"{0}{1}\".format(mean_dir, function_name)\n",
    "    create_directory(experiment_mean_dir)\n",
    "\n",
    "    for node_dir in nodes_directories:\n",
    "        file_path = \"./../experiments_data{0}{1}\".format(node_dir, function_name)\n",
    "        p = Path(file_path)\n",
    "        plots_path = \"{0}/plots\".format(file_path)\n",
    "\n",
    "        create_directory(plots_path)\n",
    "        create_directory(plots_path + \"/RAM\")\n",
    "        create_directory(plots_path + \"/CPU\")\n",
    "\n",
    "        experiment_number = 1\n",
    "        node_intervals = []\n",
    "        node_durations = []\n",
    "\n",
    "        for file in p.glob('*.csv'):\n",
    "            new_data, (duration, interval) = preprocess_file(file_path, plots_path, file.name, experiment_number)\n",
    "            base_data = pd.concat((base_data, new_data))\n",
    "\n",
    "            experiment_duration.append(duration)\n",
    "            experiment_interval.append(interval)\n",
    "            node_intervals.append(interval)\n",
    "            node_durations.append(duration)\n",
    "\n",
    "            experiment_number += 1\n",
    "\n",
    "        print(\"{0} | {1} intervals: {2}\".format(function_name[1:], node_dir[1:], node_durations))\n",
    "\n",
    "        nodes_intervals.append((node_dir[1:], round(mean(node_intervals), 3)))\n",
    "        nodes_durations.append((node_dir[1:], round(mean(node_durations), 3)))\n",
    "\n",
    "    base_data = base_data.groupby(base_data.index).mean()\n",
    "    base_data.to_csv(\"{0}/mean_data.csv\".format(experiment_mean_dir), index=False)\n",
    "\n",
    "    generate_plot(base_data.index, base_data.CPU, \"{0}/mean_CPU.png\".format(experiment_mean_dir), function_name[1:] + \" CPU\")\n",
    "    generate_plot(base_data.index, base_data.RAM, \"{0}/mean_RAM.png\".format(experiment_mean_dir), function_name[1:] + \" RAM\")\n",
    "\n",
    "    smooth_data(base_data, experiment_mean_dir, function_name[:])\n",
    "    workers_summary = workers_summary.append({\"function_name\": function_name[1:],\n",
    "                                                        \"mean_duration\": round(mean(experiment_duration), 3),\n",
    "                                                        \"mean_interval\": round(mean(experiment_interval), 3),\n",
    "                                                        \"nodes_durations\": nodes_durations,\n",
    "                                                        \"nodes_intervals\": nodes_intervals}, ignore_index=True)\n",
    "\n",
    "workers_summary.to_csv(\"{0}/experiments_mean_duration.csv\".format(mean_dir), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aggregating data from master node #20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:\n",
      "- minNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "- countNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "- sumNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "- maxNetProfitGroupedBySoldDateWhereProfitNegative - done\n",
      "- avgNetProfitGroupedBySoldDateWhereProfitNegative - done\n"
     ]
    }
   ],
   "source": [
    "create_directory(\"./../experiments_data/preprocessed-data\")\n",
    "\n",
    "master_dir = \"./../experiments_data/preprocessed-data/master-mean-data\"\n",
    "create_directory(master_dir)\n",
    "master_mean_summary = pd.DataFrame(columns=[\"function_name\",\n",
    "                                            \"mean_duration\",\n",
    "                                            \"mean_interval\"])\n",
    "\n",
    "print(\"Progress:\")\n",
    "for directory in data_directories_groups[-1]:\n",
    "    files_path = \"./../experiments_data{0}\".format(directory)\n",
    "    p = Path(files_path)\n",
    "    master_data = pd.DataFrame()\n",
    "\n",
    "    experiment_durations = []\n",
    "    experiment_intervals = []\n",
    "\n",
    "    master_experiment_directory = \"{0}{1}\".format(master_dir, directory[8:])\n",
    "    master_plot_directory = \"{0}/plots\".format(files_path)\n",
    "\n",
    "    create_directory(master_experiment_directory)\n",
    "    create_directory(master_plot_directory)\n",
    "    create_directory(\"{0}/CPU\".format(master_plot_directory))\n",
    "    create_directory(\"{0}/RAM\".format(master_plot_directory))\n",
    "\n",
    "    experiment_number = 1\n",
    "    for file in p.glob('*.csv'):\n",
    "        master_new_data, (duration, interval) = preprocess_file(files_path, master_plot_directory, file.name, experiment_number)\n",
    "        master_data = pd.concat((master_data, master_new_data))\n",
    "        experiment_durations.append(duration)\n",
    "        experiment_intervals.append(interval)\n",
    "        experiment_number += 1\n",
    "\n",
    "    master_mean_summary = master_mean_summary.append({\"function_name\": directory[9:],\n",
    "                                                      \"mean_duration\": round(mean(experiment_durations), 3),\n",
    "                                                      \"mean_interval\": round(mean(experiment_intervals), 3)},\n",
    "                                                     ignore_index=True)\n",
    "\n",
    "    master_data = master_data.groupby(master_data.index).mean()\n",
    "    master_data.to_csv(\"{0}/mean_data.csv\".format(master_experiment_directory), index=False)\n",
    "\n",
    "    generate_plot(master_data.index, master_data.CPU, \"{0}/mean_CPU.png\".format(master_experiment_directory))\n",
    "    generate_plot(master_data.index, master_data.RAM, \"{0}/mean_RAM.png\".format(master_experiment_directory))\n",
    "\n",
    "    smooth_data(master_data, master_experiment_directory, directory[9:])\n",
    "    print(\"- {0} - done\".format(directory[9:]))\n",
    "\n",
    "master_mean_summary.to_csv(\"{0}/experiments_mean_duration.csv\".format(master_dir), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing spark internal metrics\n",
    "\n",
    "Stage and task metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       function_name  stage_id  \\\n0              filterCatalogSalesWhereProfitNegative         0   \n1  filterCatalogSalesWhereProfitNegativeAndYearAf...         0   \n2  filterCatalogSalesWhereProfitNegativeAndYearAf...         1   \n3  filterCatalogSalesWhereProfitNegativeAndYearAf...         2   \n4               filterCatalogSalesWhereYearAfter2000         0   \n5               filterCatalogSalesWhereYearAfter2000         1   \n6               filterCatalogSalesWhereYearAfter2000         2   \n7                  minWholeSaleCostGroupedBySoldDate         0   \n8                  minWholeSaleCostGroupedBySoldDate         1   \n\n         task_types  mean_stage_time  num_tasks  mean_executor_run_time  \\\n0      [ResultTask]      8648.600000          1             5320.680000   \n1  [ShuffleMapTask]      4451.920000          1             1138.280000   \n2  [ShuffleMapTask]      9042.400000          1             5856.400000   \n3      [ResultTask]      4889.160000        200             2714.200000   \n4  [ShuffleMapTask]      5665.640000          1             3988.400000   \n5  [ShuffleMapTask]      4416.720000          1             1827.520000   \n6      [ResultTask]      7334.560000        200             3117.160000   \n7  [ShuffleMapTask]     73356.904762          1            69276.612245   \n8      [ResultTask]       622.564626        200              475.836735   \n\n   mean_result_size  \n0       1474.000000  \n1       2046.320000  \n2       2032.360000  \n3     820732.440000  \n4       1995.800000  \n5       2019.080000  \n6     808346.200000  \n7       2117.789116  \n8          0.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>function_name</th>\n      <th>stage_id</th>\n      <th>task_types</th>\n      <th>mean_stage_time</th>\n      <th>num_tasks</th>\n      <th>mean_executor_run_time</th>\n      <th>mean_result_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>filterCatalogSalesWhereProfitNegative</td>\n      <td>0</td>\n      <td>[ResultTask]</td>\n      <td>8648.600000</td>\n      <td>1</td>\n      <td>5320.680000</td>\n      <td>1474.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>filterCatalogSalesWhereProfitNegativeAndYearAf...</td>\n      <td>0</td>\n      <td>[ShuffleMapTask]</td>\n      <td>4451.920000</td>\n      <td>1</td>\n      <td>1138.280000</td>\n      <td>2046.320000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>filterCatalogSalesWhereProfitNegativeAndYearAf...</td>\n      <td>1</td>\n      <td>[ShuffleMapTask]</td>\n      <td>9042.400000</td>\n      <td>1</td>\n      <td>5856.400000</td>\n      <td>2032.360000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>filterCatalogSalesWhereProfitNegativeAndYearAf...</td>\n      <td>2</td>\n      <td>[ResultTask]</td>\n      <td>4889.160000</td>\n      <td>200</td>\n      <td>2714.200000</td>\n      <td>820732.440000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>filterCatalogSalesWhereYearAfter2000</td>\n      <td>0</td>\n      <td>[ShuffleMapTask]</td>\n      <td>5665.640000</td>\n      <td>1</td>\n      <td>3988.400000</td>\n      <td>1995.800000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>filterCatalogSalesWhereYearAfter2000</td>\n      <td>1</td>\n      <td>[ShuffleMapTask]</td>\n      <td>4416.720000</td>\n      <td>1</td>\n      <td>1827.520000</td>\n      <td>2019.080000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>filterCatalogSalesWhereYearAfter2000</td>\n      <td>2</td>\n      <td>[ResultTask]</td>\n      <td>7334.560000</td>\n      <td>200</td>\n      <td>3117.160000</td>\n      <td>808346.200000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>minWholeSaleCostGroupedBySoldDate</td>\n      <td>0</td>\n      <td>[ShuffleMapTask]</td>\n      <td>73356.904762</td>\n      <td>1</td>\n      <td>69276.612245</td>\n      <td>2117.789116</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>minWholeSaleCostGroupedBySoldDate</td>\n      <td>1</td>\n      <td>[ResultTask]</td>\n      <td>622.564626</td>\n      <td>200</td>\n      <td>475.836735</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_metrics_all = pd.read_csv(\"./../experiments_data/stage_metrics.csv\")\n",
    "task_metrics_all = pd.read_csv(\"./../experiments_data/task_metrics.csv\")\n",
    "\n",
    "stage_metrics = stage_metrics_all[[\"function_name\", \"stage_id\", \"num_tasks\", \"executor_run_time\", \"result_size\"]].copy()\n",
    "stage_metrics[\"stage_time\"]= stage_metrics_all[\"completion_time\"] - stage_metrics_all[\"submission_time\"]\n",
    "stage_metrics = stage_metrics.groupby([\"function_name\", \"stage_id\"]).agg({\n",
    "    \"stage_time\": \"mean\",\n",
    "    \"num_tasks\": \"max\",\n",
    "    \"executor_run_time\": \"mean\",\n",
    "    \"result_size\": \"mean\"\n",
    "})\n",
    "\n",
    "task_metrics = task_metrics_all[[\"function_name\", \"stage_id\", \"task_type\"]]\n",
    "task_metrics = task_metrics.groupby([\"function_name\", \"stage_id\"]).agg({\"task_type\": \"unique\"})\n",
    "\n",
    "spark_metrics = task_metrics.join(stage_metrics).reset_index().rename(columns={\n",
    "    \"stage_time\": \"mean_stage_time\",\n",
    "    \"task_type\": \"task_types\",\n",
    "    \"executor_run_time\": \"mean_executor_run_time\",\n",
    "    \"result_size\": \"mean_result_size\"\n",
    "})\n",
    "\n",
    "spark_metrics.to_csv(\"./../experiments_data/preprocessed-data/spark_metrics.csv\", index=False)\n",
    "spark_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clear preprocessed data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ./../experiments_data/node_17/minNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_20/minNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_13/minNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_18/minNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_19/minNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_16/minNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_11/minNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_15/minNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_14/minNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_17/countNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_20/countNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_13/countNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_18/countNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_19/countNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_16/countNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_11/countNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_15/countNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_14/countNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_17/sumNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_20/sumNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_13/sumNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_18/sumNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_19/sumNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_16/sumNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_11/sumNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_15/sumNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_14/sumNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_17/maxNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_20/maxNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_13/maxNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_18/maxNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_19/maxNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_16/maxNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_11/maxNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_15/maxNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_14/maxNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_17/avgNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_20/avgNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_13/avgNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_18/avgNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_19/avgNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_16/avgNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_11/avgNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_15/avgNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n",
      "Error: ./../experiments_data/node_14/avgNetProfitGroupedBySoldDateWhereProfitNegative/plots - No such file or directory.\n"
     ]
    }
   ],
   "source": [
    "for function_name in function_names:\n",
    "    for node_dir in nodes_directories:\n",
    "        try:\n",
    "            shutil.rmtree('./../experiments_data{}{}/plots'.format(node_dir, function_name))\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('./../experiments_data/preprocessed-data')\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}