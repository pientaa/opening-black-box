{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "prepare_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "pycharm-afd99e2c",
   "language": "python",
   "display_name": "PyCharm (opening-black-box)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "FQnQig-Tqwrs"
   },
   "source": [
    "import pandas as pd   \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "from statistics import mean\n"
   ],
   "execution_count": 103,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook for preprocessing data from experiments\n",
    "\n",
    "Defined functions:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "def get_subdirectories(directory=\"\"):\n",
    "    subdirectories = []\n",
    "    p = Path(\"./../experiments_data/\" + directory)\n",
    "    for item in p.glob('*/'):\n",
    "        if item.suffix not in (['.csv', '.zip']):\n",
    "            subdirectories.append(directory + \"/\" + item.name)\n",
    "    return subdirectories\n",
    "\n",
    "def get_duration(dataframe):\n",
    "    start_time = dataframe['timestamp'].max()\n",
    "    stop_time = dataframe['timestamp'].min()\n",
    "\n",
    "    start_datetime = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    stop_datetime = datetime.strptime(stop_time, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    duration = stop_datetime - start_datetime\n",
    "    return duration.microseconds\n",
    "\n",
    "def preprocess_file(file_path, save_path, filename, iteration):\n",
    "    read_data = pd.read_csv(file_path + \"/\" + filename)\n",
    "    agg_read_data = read_data.\\\n",
    "        groupby('timestamp', as_index=False).\\\n",
    "        agg({\"CPU\": \"sum\", \"RAM\": \"sum\"})\n",
    "\n",
    "    generate_plot(agg_read_data.index, agg_read_data.CPU, \"{0}/CPU/{1}_CPU.png\".format(save_path, iteration))\n",
    "    generate_plot(agg_read_data.index, agg_read_data.RAM, \"{0}/RAM/{1}_RAM.png\".format(save_path, iteration))\n",
    "\n",
    "    duration = get_duration(agg_read_data)\n",
    "\n",
    "    return agg_read_data, duration\n",
    "\n",
    "def generate_plot(data_x, data_y, plot_path, title=\"\"):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.plot(data_x, data_y, 'r.-')\n",
    "    plt.title(title)\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "def create_directory(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check for subdirectories. Number of subdirectories should much the number of nodes used for experiments."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "['/node_11',\n '/node_12',\n '/node_13',\n '/node_14',\n '/node_15',\n '/node_16',\n '/node_17',\n '/node_18',\n '/node_19',\n '/node_20']"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_directories = get_subdirectories()\n",
    "nodes_directories = [x for x in all_directories if \"node\" in x]\n",
    "nodes_directories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "[['/node_11/filterCatalogSalesWhereProfitNegative',\n  '/node_11/filterCatalogSalesWhereProfitNegativeAndYearAfter2000',\n  '/node_11/filterCatalogSalesWhereYearAfter2000',\n  '/node_11/minWholeSaleCostGroupedBySoldDate'],\n ['/node_12/filterCatalogSalesWhereProfitNegative',\n  '/node_12/filterCatalogSalesWhereProfitNegativeAndYearAfter2000',\n  '/node_12/filterCatalogSalesWhereYearAfter2000',\n  '/node_12/minWholeSaleCostGroupedBySoldDate'],\n ['/node_13/filterCatalogSalesWhereProfitNegative',\n  '/node_13/filterCatalogSalesWhereProfitNegativeAndYearAfter2000',\n  '/node_13/filterCatalogSalesWhereYearAfter2000',\n  '/node_13/minWholeSaleCostGroupedBySoldDate'],\n ['/node_14/filterCatalogSalesWhereProfitNegative',\n  '/node_14/filterCatalogSalesWhereProfitNegativeAndYearAfter2000',\n  '/node_14/filterCatalogSalesWhereYearAfter2000',\n  '/node_14/minWholeSaleCostGroupedBySoldDate'],\n ['/node_15/filterCatalogSalesWhereProfitNegative',\n  '/node_15/filterCatalogSalesWhereProfitNegativeAndYearAfter2000',\n  '/node_15/filterCatalogSalesWhereYearAfter2000',\n  '/node_15/minWholeSaleCostGroupedBySoldDate'],\n ['/node_16/filterCatalogSalesWhereProfitNegative',\n  '/node_16/filterCatalogSalesWhereProfitNegativeAndYearAfter2000',\n  '/node_16/filterCatalogSalesWhereYearAfter2000',\n  '/node_16/minWholeSaleCostGroupedBySoldDate'],\n ['/node_17/filterCatalogSalesWhereProfitNegative',\n  '/node_17/filterCatalogSalesWhereProfitNegativeAndYearAfter2000',\n  '/node_17/filterCatalogSalesWhereYearAfter2000',\n  '/node_17/minWholeSaleCostGroupedBySoldDate'],\n ['/node_18/filterCatalogSalesWhereProfitNegative',\n  '/node_18/filterCatalogSalesWhereProfitNegativeAndYearAfter2000',\n  '/node_18/filterCatalogSalesWhereYearAfter2000',\n  '/node_18/minWholeSaleCostGroupedBySoldDate'],\n ['/node_19/filterCatalogSalesWhereProfitNegative',\n  '/node_19/filterCatalogSalesWhereProfitNegativeAndYearAfter2000',\n  '/node_19/filterCatalogSalesWhereYearAfter2000',\n  '/node_19/minWholeSaleCostGroupedBySoldDate'],\n ['/node_20/filterCatalogSalesWhereProfitNegative',\n  '/node_20/filterCatalogSalesWhereProfitNegativeAndYearAfter2000',\n  '/node_20/filterCatalogSalesWhereYearAfter2000',\n  '/node_20/minWholeSaleCostGroupedBySoldDate']]"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_directories = []\n",
    "data_directories_groups = []\n",
    "for directory in nodes_directories:\n",
    "    cur_node_subdirectories = get_subdirectories(directory)\n",
    "    data_directories.append(cur_node_subdirectories)\n",
    "\n",
    "data_directories_groups = data_directories\n",
    "data_directories = [item for sublist in data_directories for item in sublist]\n",
    "data_directories_groups"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check for function names in each node directory.\n",
    "These names should match the names in `experiments-plan.csv`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "['/filterCatalogSalesWhereProfitNegative',\n '/filterCatalogSalesWhereProfitNegativeAndYearAfter2000',\n '/filterCatalogSalesWhereYearAfter2000',\n '/minWholeSaleCostGroupedBySoldDate']"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_names = data_directories_groups[1]\n",
    "function_names = list(map(lambda x: x[8:], data_directories_groups[1]))\n",
    "function_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aggregating data from all nodes (11-19)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_directory(\"./../experiments_data/preprocessed-data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_dir = \"./../experiments_data/preprocessed-data/workers-mean-data\"\n",
    "create_directory(mean_dir)\n",
    "\n",
    "for function_name in function_names[0:1]:\n",
    "    base_data = pd.DataFrame()\n",
    "    experiment_mean_dir = \"{0}{1}\".format(mean_dir, function_name)\n",
    "\n",
    "    create_directory(experiment_mean_dir)\n",
    "\n",
    "    for node_dir in nodes_directories[:-1]:\n",
    "        file_path = \"./../experiments_data{0}{1}\".format(node_dir, function_name)\n",
    "        p = Path(file_path)\n",
    "        plots_path = \"{0}/plots\".format(file_path)\n",
    "        create_directory(plots_path)\n",
    "        create_directory(plots_path + \"/RAM\")\n",
    "        create_directory(plots_path + \"/CPU\")\n",
    "        experiment_number = 1\n",
    "        for file in p.glob('*.csv'):\n",
    "            new_data, duration = preprocess_file(file_path, plots_path, file.name, experiment_number)\n",
    "            base_data = pd.concat((base_data, new_data))\n",
    "            experiment_number += 1\n",
    "\n",
    "    base_data = base_data.groupby(base_data.index).mean()\n",
    "    base_data.to_csv(\"{0}/mean_data.csv\".format(experiment_mean_dir), index=False)\n",
    "\n",
    "    generate_plot(base_data.index, base_data.CPU, \"{0}/avg_CPU.png\".format(experiment_mean_dir), function_name[1:] + \" CPU\")\n",
    "    generate_plot(base_data.index, base_data.RAM, \"{0}/avg_RAM.png\".format(experiment_mean_dir), function_name[1:] + \" RAM\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aggregating data from master node #20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "master_dir = \"./../experiments_data/preprocessed-data/master-mean-data\"\n",
    "create_directory(master_dir)\n",
    "\n",
    "for directory in data_directories_groups[-1]:\n",
    "    files_path = \"./../experiments_data{0}\".format(directory)\n",
    "    p = Path(files_path)\n",
    "    master_data = pd.DataFrame()\n",
    "\n",
    "    master_experiment_directory = \"{0}{1}\".format(master_dir, directory[8:])\n",
    "    master_plot_directory = \"{0}/plots\".format(files_path)\n",
    "    create_directory(master_experiment_directory)\n",
    "    create_directory(master_plot_directory)\n",
    "    create_directory(\"{0}/CPU\".format(master_plot_directory))\n",
    "    create_directory(\"{0}/RAM\".format(master_plot_directory))\n",
    "\n",
    "    experiment_number = 1\n",
    "    for file in p.glob('*.csv'):\n",
    "        master_new_data, duration = preprocess_file(files_path, master_plot_directory, file.name, experiment_number)\n",
    "        master_data = pd.concat((master_data, master_new_data))\n",
    "        experiment_number += 1\n",
    "\n",
    "    master_data = master_data.groupby(master_data.index).mean()\n",
    "    master_data.to_csv(\"{0}/mean_data.csv\".format(master_experiment_directory), index=False)\n",
    "\n",
    "    generate_plot(master_data.index, master_data.CPU, \"{0}/avg_CPU.png\".format(master_experiment_directory))\n",
    "    generate_plot(master_data.index, master_data.RAM, \"{0}/avg_RAM.png\".format(master_experiment_directory))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing spark internal metrics\n",
    "\n",
    "Stage and task metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stage_metrics_all = pd.read_csv(\"./../experiments_data/stage_metrics.csv\")\n",
    "task_metrics_all = pd.read_csv(\"./../experiments_data/task_metrics.csv\")\n",
    "\n",
    "stage_metrics = stage_metrics_all[[\"function_name\", \"stage_id\", \"num_tasks\", \"executor_run_time\", \"result_size\"]].copy()\n",
    "stage_metrics[\"stage_time\"]= stage_metrics_all[\"completion_time\"] - stage_metrics_all[\"submission_time\"]\n",
    "stage_metrics = stage_metrics.groupby([\"function_name\", \"stage_id\"]).agg({\n",
    "    \"stage_time\": \"mean\",\n",
    "    \"num_tasks\": \"max\",\n",
    "    \"executor_run_time\": \"mean\",\n",
    "    \"result_size\": \"mean\"\n",
    "})\n",
    "\n",
    "task_metrics = task_metrics_all[[\"function_name\", \"stage_id\", \"task_type\"]]\n",
    "task_metrics = task_metrics.groupby([\"function_name\", \"stage_id\"]).agg({\"task_type\": \"unique\"})\n",
    "\n",
    "spark_metrics = task_metrics.join(stage_metrics).reset_index().rename(columns={\n",
    "    \"stage_time\": \"mean_stage_time\",\n",
    "    \"task_type\": \"task_types\",\n",
    "    \"executor_run_time\": \"mean_executor_run_time\",\n",
    "    \"result_size\": \"mean_result_size\"\n",
    "})\n",
    "\n",
    "spark_metrics.to_csv(\"./../experiments_data/preprocessed-data/spark_metrics.csv\", index=False)\n",
    "spark_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clear preprocessed data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for function_name in function_names:\n",
    "    for node_dir in nodes_directories:\n",
    "        try:\n",
    "            shutil.rmtree('./../experiments_data{}{}/plots'.format(node_dir, function_name))\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('./../experiments_data/preprocessed-data')\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}